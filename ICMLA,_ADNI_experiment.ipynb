{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICMLA, ADNI experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gNhcNKVxvWWO"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Ig1R3lr3VX",
        "outputId": "9f4e6eae-f8e5-44cd-a009-89840f936ad1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul  7 03:09:18 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzBA5p3MsWJt",
        "outputId": "383ff66c-dbb1-4583-c311-c79ab3fe48e8"
      },
      "source": [
        "!pip install missingpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: missingpy in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwnxQ_IlEoce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809a0f37-132d-4aee-d368-4f48e79d25c5"
      },
      "source": [
        "pip install neural-tangents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: neural-tangents in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: jax>=0.1.77 in /usr/local/lib/python3.7/dist-packages (from neural-tangents) (0.2.13)\n",
            "Requirement already satisfied: frozendict>=1.2 in /usr/local/lib/python3.7/dist-packages (from neural-tangents) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.77->neural-tangents) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.77->neural-tangents) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.77->neural-tangents) (3.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.1.77->neural-tangents) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gqR7bKgQrEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6ad488-a1f3-484c-c29a-6eb77f0030a1"
      },
      "source": [
        "# from MICE import *\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "from matplotlib.pyplot import figure\n",
        "import statsmodels.api as sm\n",
        "import time\n",
        "\n",
        "import jax.numpy\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import stax\n",
        "from jax import random\n",
        "# from jax.experimental import optimizers\n",
        "# from jax.api import jit, grad, vmap\n",
        "import functools\n",
        "\n",
        "import math\n",
        "import copy\n",
        "import statistics\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import *\n",
        "from sklearn.svm import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghsmJIRwupOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c5e749-4354-4259-df81-ee261a673093"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Data/adni_org.csv')\n",
        "df = df.drop(['PTGENDER','DX_bl','PTRACCAT'], axis=1)\n",
        "df = df.drop(df.columns[0], axis=1)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        8355  126282   92736  284099  ...   26791   8683  595098  MNI_Hippocampus_R\n",
            "0    2.30525  5.1490  2.4235   2.268  ...  11.887  9.430  11.826           0.467067\n",
            "1    2.19000  4.8195  2.4490   2.130  ...  12.063  9.734  11.952           0.421698\n",
            "2    2.23900  5.4440  2.7675   1.957  ...  11.506  9.578  11.949           0.383664\n",
            "3    2.25450  5.3955  2.6865   2.347  ...  11.240  9.504  11.901           0.452964\n",
            "4    2.18650  5.3405  2.5875   2.154  ...  11.524  9.865  11.959           0.427152\n",
            "..       ...     ...     ...     ...  ...     ...    ...     ...                ...\n",
            "644  2.19550  5.5090  2.6485   2.261  ...  11.640  9.508  11.905           0.354836\n",
            "645  2.31375  5.1385  2.8110   2.201  ...  11.585  9.666  11.882           0.440644\n",
            "646  2.32375  5.1670  2.5785   2.205  ...  11.395  9.401  11.969           0.506813\n",
            "647  2.33300  5.5065  2.4100   2.161  ...  11.769  9.368  11.942           0.403784\n",
            "648  2.37150  5.5250  2.6460   2.130  ...  11.691  9.342  11.923           0.477733\n",
            "\n",
            "[649 rows x 19823 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL2IJy2W7Y7J",
        "outputId": "9bdcee23-ad76-44c9-c07f-1f74bc474fe6"
      },
      "source": [
        "scaler = StandardScaler(with_mean=True, with_std=False).fit(df.iloc[:,0:-1])\n",
        "df.iloc[:,0:-1] = scaler.transform(df.iloc[:,0:-1])\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         8355   126282     92736  ...      8683  595098  MNI_Hippocampus_R\n",
            "0    0.016239 -0.18876 -0.128827  ... -0.160445 -0.0531           0.467067\n",
            "1   -0.099011 -0.51826 -0.103327  ...  0.143555  0.0729           0.421698\n",
            "2   -0.050011  0.10624  0.215173  ... -0.012445  0.0699           0.383664\n",
            "3   -0.034511  0.05774  0.134173  ... -0.086445  0.0219           0.452964\n",
            "4   -0.102511  0.00274  0.035173  ...  0.274555  0.0799           0.427152\n",
            "..        ...      ...       ...  ...       ...     ...                ...\n",
            "644 -0.093511  0.17124  0.096173  ... -0.082445  0.0259           0.354836\n",
            "645  0.024739 -0.19926  0.258673  ...  0.075555  0.0029           0.440644\n",
            "646  0.034739 -0.17076  0.026173  ... -0.189445  0.0899           0.506813\n",
            "647  0.043989  0.16874 -0.142327  ... -0.222445  0.0629           0.403784\n",
            "648  0.082489  0.18724  0.093673  ... -0.248445  0.0439           0.477733\n",
            "\n",
            "[649 rows x 19823 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB5GPc2S1m4a"
      },
      "source": [
        "dfy = df['MNI_Hippocampus_R']\n",
        "cor = np.zeros(19822)\n",
        "for i in range(19822):\n",
        "  dfx_col = df.iloc[:, i] \n",
        "  cor[i] = dfx_col.corr(dfy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRc4NnLhdddu",
        "outputId": "4343e8ab-6f5b-4c91-dd9b-2f31425866e6"
      },
      "source": [
        "data = df.to_numpy()\n",
        "p_f=p=1500\n",
        "X = data[:,cor.argsort()[-p:][::-1]]\n",
        "y = data[:,-1].reshape((-1,1))\n",
        "print(X.shape, y.shape)\n",
        "adni_data = np.concatenate((X, y), axis=1)\n",
        "print(adni_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(649, 1500) (649, 1)\n",
            "(649, 1501)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN-PZMu8liDe",
        "outputId": "f96aa268-be94-40e3-dd87-063e92632f81"
      },
      "source": [
        "n=adni_data.shape[0]\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
        "print(device)\n",
        "\n",
        "np.random.seed(1)\n",
        "true_beta=np.zeros(p)\n",
        "q=[0,1,2]\n",
        "# q=[0,1,2,3,4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRRUmKuymGUW"
      },
      "source": [
        "def data_G_linear(n,p,data,seed=1,a0=0,a1=0.1,a2=0.1,a3=0,a4=0.1,a5=0):\n",
        "  np.random.seed(seed) \n",
        "  sample = data.copy()\n",
        "  missing_col1 = np.zeros(p+1)\n",
        "  missing_col2 = np.zeros(p+1)\n",
        "  if p==1000:\n",
        "    missing_col1[0:200]=1\n",
        "    missing_col2[200:400]=1\n",
        "  elif p==1500:\n",
        "    missing_col1[0:200]=1\n",
        "    missing_col2[200:400]=1\n",
        "  elif p==10000:\n",
        "    missing_col1[0:1000]=1\n",
        "    missing_col2[1000:2000]=1\n",
        "    # missing_col1[0:100]=1\n",
        "    # missing_col1[600:1000]=1\n",
        "    # missing_col2[1500:2000]=1\n",
        "  elif p==19822:\n",
        "    # missing_col1[0:2000]=1\n",
        "    # missing_col2[2000:4000]=1\n",
        "    missing_col1[0:1000]=1\n",
        "    missing_col2[1000:2000]=1\n",
        "  y=sample[:,-1]\n",
        "  ###  MNAR  ###\n",
        "  # logit1=a0+a1*np.mean(sample[:,missing_col2==1][:,0:5],axis=1)+a2*y\n",
        "  # logit2=a3+a4*np.mean(sample[:,missing_col1==1][:,0:5],axis=1)+a5*y\n",
        "  ###  MAR  ###\n",
        "  logit1=a0+a1*np.mean(sample[:,np.logical_or(missing_col1,missing_col2)==0][:,0:100],axis=1)+a2*y\n",
        "  logit2=a3+a4*np.mean(sample[:,np.logical_or(missing_col1,missing_col2)==0][:,200:300],axis=1)+a5*y\n",
        "  p_miss1 = 1/(1+np.exp(-logit1))  \n",
        "  p_miss2 = 1/(1+np.exp(-logit2))  \n",
        "  missing_row1=np.zeros((n,))\n",
        "  missing_row2=np.zeros((n,))\n",
        "  for i in range(n):\n",
        "    missing_row1[i]= np.random.choice(2, 1, p=[1-p_miss1[i],p_miss1[i]])\n",
        "    missing_row2[i]= np.random.choice(2, 1, p=[1-p_miss2[i],p_miss2[i]])\n",
        "\n",
        "  return sample,missing_row1==1,missing_row2==1,missing_col1==1,missing_col2==1,y.reshape((-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uw0VLLJdPhf"
      },
      "source": [
        "def data_to_stats(imputed_data_list,truth1,truth2,rows1,rows2,cols1,cols2):\n",
        "  # for real data analysis, there is no true_bata\n",
        "  rounds=len(imputed_data_list)\n",
        "  MSE=0\n",
        "  MEANs=np.zeros((rounds,4))\n",
        "  within_VAR=np.zeros((4,4))\n",
        "  # MEANs=np.zeros((rounds,6))\n",
        "  # within_VAR=np.zeros((6,6))\n",
        "\n",
        "  for i in range(rounds):\n",
        "    imputed_data_MICE=imputed_data_list[i]\n",
        "    X=imputed_data_MICE[:,:-1]\n",
        "    ols = sm.OLS(imputed_data_MICE[:,-1], sm.add_constant(X[:,[0,1,2]]))\n",
        "    # ols = sm.OLS(imputed_data_MICE[:,-1], sm.add_constant(X[:,[0,1,2,3,4]]))\n",
        "    result=ols.fit()\n",
        "    within_VAR+=result.cov_params()/rounds\n",
        "    MEANs[i,:]=result.params\n",
        "    MSE+=np.sum((imputed_data_MICE[rows1,:][:,cols1]-truth1)**2)/(rounds*(np.sum(rows1)*np.sum(cols1)+np.sum(rows2)*np.sum(cols2)))\n",
        "    MSE+=np.sum((imputed_data_MICE[rows2,:][:,cols2]-truth2)**2)/(rounds*(np.sum(rows1)*np.sum(cols1)+np.sum(rows2)*np.sum(cols2)))\n",
        "  \n",
        "  if rounds==1:\n",
        "    VAR=within_VAR\n",
        "    MEAN=MEANs[0,:]\n",
        "  else:\n",
        "    # Rubin's rule for total variance\n",
        "    MEAN=np.mean(MEANs,axis=0)\n",
        "    Betw_VAR=np.zeros((4,4))\n",
        "    # Betw_VAR=np.zeros((6,6))\n",
        "    # between imputation variance\n",
        "    for i in range(rounds):\n",
        "        temp=MEANs[i,:]-MEAN\n",
        "        Betw_VAR+=np.outer(temp,temp)/(rounds-1)\n",
        "    VAR=within_VAR+(1+1/rounds)*Betw_VAR\n",
        "  return MSE,MEAN,VAR,np.sqrt(VAR[1][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtKKAJx_Z2lj"
      },
      "source": [
        "# ***benchmark***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFmgPgEJEp6K"
      },
      "source": [
        "full_SD=[]\n",
        "partial_SD=[]\n",
        "naive_imp_SD=[]\n",
        "\n",
        "full_SE=0\n",
        "partial_SE=0\n",
        "naive_imp_SE=0\n",
        "\n",
        "naive_imp_MSE=0\n",
        "\n",
        "full_beta_estimate=0\n",
        "partial_beta_estimate=0\n",
        "naive_imp_beta_estimate=0\n",
        "\n",
        "num_of_missing_row = 0\n",
        "num_of_missing_row1 = 0\n",
        "num_of_missing_row2 = 0\n",
        "\n",
        "a0=-1\n",
        "a1=-3\n",
        "a2=3\n",
        "a3=-1\n",
        "a4=-3\n",
        "a5=2\n",
        "Monte_Carlo =100\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  sample,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,data=adni_data,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5)\n",
        "  # print(sum(missing_row))\n",
        "  truth1=sample[missing_row1==True,:][:,missing_col1==True].copy()\n",
        "  truth2=sample[missing_row2==True,:][:,missing_col2==True].copy()\n",
        "\n",
        "  complete_data=sample.copy()\n",
        "  complete_case=sample[np.logical_or(missing_row1,missing_row2)==0,:].copy()\n",
        "  # print(complete_case.shape)\n",
        "  \n",
        "  num_mis = np.sum(np.logical_or(missing_row1, missing_row2))\n",
        "  num_mis1 = np.sum(missing_row1)\n",
        "  num_mis2 = np.sum(missing_row2)\n",
        "  num_of_missing_row1 = (num_of_missing_row1*(iteration-1)+num_mis1)/iteration\n",
        "  num_of_missing_row2 = (num_of_missing_row2*(iteration-1)+num_mis2)/iteration\n",
        "  num_of_missing_row = (num_of_missing_row*(iteration-1)+num_mis)/iteration\n",
        "  ########## benchmark: full data\n",
        "  mse,b,c,se=data_to_stats([complete_data],truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  \n",
        "  full_beta_estimate=(full_beta_estimate*(iteration-1)+b[1])/iteration\n",
        "  full_SE=(full_SE*(iteration-1)+se)/iteration\n",
        "  full_SD.append(b[1])\n",
        "\n",
        "  ########## benchmark: partial\n",
        "  X=complete_case[:,:-1]\n",
        "  ols = sm.OLS(complete_case[:,-1], sm.add_constant(X[:,[0,1,2]]))\n",
        "  # ols = sm.OLS(complete_case[:,-1], sm.add_constant(X[:,[0,1,2,3,4]]))\n",
        "  result=ols.fit()\n",
        "  c=result.cov_params()\n",
        "  b=result.params\n",
        "  \n",
        "  partial_beta_estimate=(partial_beta_estimate*(iteration-1)+b[1])/iteration\n",
        "  partial_SE=(partial_SE*(iteration-1)+np.sqrt(c[1][1]))/iteration\n",
        "  partial_SD.append(b[1])\n",
        "\n",
        "  ########## benchmark: naive imputation\n",
        "  naive_impute = sample.copy()\n",
        "  mis1 = naive_impute[:,missing_col1==True]\n",
        "  mis1[missing_row1,:]=np.mean(mis1[missing_row1==0,:], axis=0)\n",
        "  mis2 = naive_impute[:,missing_col2==True]\n",
        "  mis2[missing_row2,:]=np.mean(mis2[missing_row2==0,:], axis=0)\n",
        "  naive_impute[:,missing_col1==True] = mis1\n",
        "  naive_impute[:,missing_col2==True] = mis2\n",
        "\n",
        "  mse,b,c,se=data_to_stats([naive_impute],truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  \n",
        "  naive_imp_beta_estimate=(naive_imp_beta_estimate*(iteration-1)+b[1])/iteration\n",
        "  naive_imp_SE=(naive_imp_SE*(iteration-1)+se)/iteration\n",
        "  naive_imp_SD.append(b[1])\n",
        "  naive_imp_MSE=(naive_imp_MSE*(iteration-1)+mse)/iteration\n",
        "\n",
        "print('full_MSE: NA ', 'full_estimate ', full_beta_estimate, 'full_SE: ',full_SE)\n",
        "print('partial_MSE: NA ', 'partial_estimate ', partial_beta_estimate, 'partial_SE: ',partial_SE)\n",
        "print('naive_imp_MSE: ', naive_imp_MSE, 'naive_imp_estimate ', naive_imp_beta_estimate, 'naive_imp_SE: ',naive_imp_SE)\n",
        "full_StD=np.std(full_SD, dtype=np.float64)\n",
        "partial_StD=np.std(partial_SD, dtype=np.float64)\n",
        "naive_imp_StD=np.std(naive_imp_SD, dtype=np.float64)\n",
        "print('full_StD: ', full_StD)\n",
        "print('partial_StD: ', partial_StD)\n",
        "print('naive_imp_StD: ', naive_imp_StD)\n",
        "\n",
        "# missing rate\n",
        "print('num_of_missing_row: ',num_of_missing_row)\n",
        "print('num_of_missing_row1: ',num_of_missing_row1)\n",
        "print('num_of_missing_row2: ',num_of_missing_row2)\n",
        "\n",
        "print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo)\n",
        "print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5, 'q=',q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNhcNKVxvWWO"
      },
      "source": [
        "# ***GAIN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApBVqPPEvdOd",
        "outputId": "799cc301-a055-4caa-e3e5-4e00e5810ed1"
      },
      "source": [
        "from gain import gain\n",
        "from utils import rmse_loss\n",
        "\n",
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_estimate=0\n",
        "Model_2_MSE=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=-1\n",
        "a1=-3\n",
        "a2=3\n",
        "a3=-1\n",
        "a4=-3\n",
        "a5=2\n",
        "Monte_Carlo =50\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  sample,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,data=adni_data,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5)\n",
        "  # print(sum(missing_row))\n",
        "  truth1=sample[missing_row1==True,:][:,missing_col1==True].copy()\n",
        "  truth2=sample[missing_row2==True,:][:,missing_col2==True].copy()\n",
        "  \n",
        "  s=time.time()\n",
        "  \n",
        "  mis1 = sample[:,missing_col1==True].copy()\n",
        "  mis2 = sample[:,missing_col2==True].copy()\n",
        "  mis1[missing_row1==True,:] = np.nan\n",
        "  mis2[missing_row2==True,:] = np.nan\n",
        "  sample[:,missing_col1==True]=mis1\n",
        "  sample[:,missing_col2==True]=mis2\n",
        "  rawdata = sample.copy()\n",
        "\n",
        "  gain_parameters = {'batch_size': 256,\n",
        "                    'hint_rate': 0.9,\n",
        "                    'alpha': 100.0,\n",
        "                    'iterations': 5000}\n",
        "  imputed_data_x = gain(rawdata.copy(), gain_parameters)\n",
        "\n",
        "  mse,b,c,se=data_to_stats([imputed_data_x],truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Model_2_estimate: ',b[1])\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[1])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_estimate=(Model_2_estimate*(iteration-1)+b[1])/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_estimate: ',Model_2_estimate,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    print('Model_2 average cost:', Model_2_time)\n",
        "\n",
        "print('n:', n, ' p: ',p, ' Monte Carlo:', Monte_Carlo)\n",
        "print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:45<00:00, 30.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  169.07143878936768  sec \n",
            " MSE: 1.2490769020804864 ; Model_2_estimate:  0.011254740462352626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:52<00:00, 29.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  173.55925488471985  sec \n",
            " MSE: 1.176442556518863 ; Model_2_estimate:  0.01871784341186143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:43<00:00, 30.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  164.68632340431213  sec \n",
            " MSE: 1.187892967376695 ; Model_2_estimate:  0.02633590289317979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [03:09<00:00, 26.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  190.89819860458374  sec \n",
            " MSE: 1.2261540334594963 ; Model_2_estimate:  0.010510616946050017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:43<00:00, 30.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  165.6960747241974  sec \n",
            " MSE: 1.2138264249509945 ; Model_2_estimate:  0.022103927286306897\n",
            "iteration:  5 Model_2_MSE:  1.210678576877307 Model_2_estimate:  0.017784606199950148 Model_2_SE:  0.011984995296703327\n",
            "Model_2_StD:  0.0061351806443378145\n",
            "Model_2 average cost: 172.78271236419678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:45<00:00, 30.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  167.51516199111938  sec \n",
            " MSE: 1.3249597847135055 ; Model_2_estimate:  0.0321380261740608\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:44<00:00, 30.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  166.61416292190552  sec \n",
            " MSE: 1.2824376403439741 ; Model_2_estimate:  0.007259017132035791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:45<00:00, 30.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  168.67868661880493  sec \n",
            " MSE: 1.2250637363184618 ; Model_2_estimate:  0.0011407465733792451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:42<00:00, 30.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  165.73152041435242  sec \n",
            " MSE: 1.248711989552591 ; Model_2_estimate:  0.024702321333157615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:44<00:00, 30.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  166.97790670394897  sec \n",
            " MSE: 1.226193763520525 ; Model_2_estimate:  0.034530409401732534\n",
            "iteration:  10 Model_2_MSE:  1.2360759798835592 Model_2_estimate:  0.018869355161411673 Model_2_SE:  0.01145855979810683\n",
            "Model_2_StD:  0.010481120482015245\n",
            "Model_2 average cost: 169.94343383312224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:44<00:00, 30.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  167.42819714546204  sec \n",
            " MSE: 1.3028483182720636 ; Model_2_estimate:  0.016799884057637684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:43<00:00, 30.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  166.84772849082947  sec \n",
            " MSE: 1.1898978966910096 ; Model_2_estimate:  0.014554868784765737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [03:02<00:00, 27.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  186.0490279197693  sec \n",
            " MSE: 1.2509955489555407 ; Model_2_estimate:  0.023465261700523432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:43<00:00, 30.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  167.2388527393341  sec \n",
            " MSE: 1.1898565782579287 ; Model_2_estimate:  0.02001751888079411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:44<00:00, 30.35it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  168.96991729736328  sec \n",
            " MSE: 1.148871101020922 ; Model_2_estimate:  0.035428945588987945\n",
            "iteration:  15 Model_2_MSE:  1.2295486161355371 Model_2_estimate:  0.019930668708455043 Model_2_SE:  0.01181764186698349\n",
            "Model_2_StD:  0.009665061628441484\n",
            "Model_2 average cost: 170.39826844533283\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:44<00:00, 30.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  168.43179750442505  sec \n",
            " MSE: 1.36285161533201 ; Model_2_estimate:  0.023799280227033023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:44<00:00, 30.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  169.19992661476135  sec \n",
            " MSE: 1.2726968063959894 ; Model_2_estimate:  0.018836796336112883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:45<00:00, 30.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  169.89142060279846  sec \n",
            " MSE: 1.3499090355492291 ; Model_2_estimate:  0.03489971087248896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:43<00:00, 30.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  168.37274265289307  sec \n",
            " MSE: 1.2739934829862114 ; Model_2_estimate:  0.026437256114669648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:46<00:00, 30.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  171.4895749092102  sec \n",
            " MSE: 1.2313996235928029 ; Model_2_estimate:  0.02690783627824299\n",
            "iteration:  20 Model_2_MSE:  1.2467039902944648 Model_2_estimate:  0.021492045522768656 Model_2_SE:  0.011617783627734048\n",
            "Model_2_StD:  0.00917522418044088\n",
            "Model_2 average cost: 170.16827156543732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:42<00:00, 30.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  168.29149317741394  sec \n",
            " MSE: 1.2610925633842656 ; Model_2_estimate:  0.029722967328730043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:39<00:00, 31.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  165.29307985305786  sec \n",
            " MSE: 1.3381171098661102 ; Model_2_estimate:  0.02617631340685463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:43<00:00, 30.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  169.21547842025757  sec \n",
            " MSE: 1.2548322069090332 ; Model_2_estimate:  0.006301265708938599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:43<00:00, 30.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  169.5451591014862  sec \n",
            " MSE: 1.1854130108007865 ; Model_2_estimate:  0.011175715542916798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:42<00:00, 30.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  169.16081428527832  sec \n",
            " MSE: 1.2690538053226534 ; Model_2_estimate:  0.03167950812006835\n",
            "iteration:  25 Model_2_MSE:  1.249703540086886 Model_2_estimate:  0.021395867222515262 Model_2_SE:  0.010957572742327608\n",
            "Model_2_StD:  0.009410897529467669\n",
            "Model_2 average cost: 169.79506542205812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:42<00:00, 30.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  169.62779903411865  sec \n",
            " MSE: 1.2072573274780036 ; Model_2_estimate:  0.010235032249556023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:44<00:00, 30.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  171.39350414276123  sec \n",
            " MSE: 1.220889419084049 ; Model_2_estimate:  0.011352431013600372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:43<00:00, 30.58it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  170.7886824607849  sec \n",
            " MSE: 1.1404140188768435 ; Model_2_estimate:  0.0018974433120595253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:42<00:00, 30.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  170.32334160804749  sec \n",
            " MSE: 1.2504592312703346 ; Model_2_estimate:  0.006386955787071453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:37<00:00, 31.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  165.80827736854553  sec \n",
            " MSE: 1.1766478045294921 ; Model_2_estimate:  0.027948131216536384\n",
            "iteration:  30 Model_2_MSE:  1.241275210113696 Model_2_estimate:  0.019757222471390178 Model_2_SE:  0.011040200881198345\n",
            "Model_2_StD:  0.010012314215062318\n",
            "Model_2 average cost: 169.76078429222108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:39<00:00, 31.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  167.9287896156311  sec \n",
            " MSE: 1.2382854070036609 ; Model_2_estimate:  0.0015477536354274596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:36<00:00, 31.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  165.08807158470154  sec \n",
            " MSE: 1.1619810863008082 ; Model_2_estimate:  0.024469973030060578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:36<00:00, 31.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  165.49479222297668  sec \n",
            " MSE: 1.180976101411198 ; Model_2_estimate:  0.013057107713901254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:37<00:00, 31.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  167.4968671798706  sec \n",
            " MSE: 1.253489331253021 ; Model_2_estimate:  0.016199587020793924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:33<00:00, 32.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  163.8847861289978  sec \n",
            " MSE: 1.1815435887999777 ; Model_2_estimate:  0.004058745912578156\n",
            "iteration:  35 Model_2_MSE:  1.235843766233701 Model_2_estimate:  0.01862999547012762 Model_2_SE:  0.010926168735702258\n",
            "Model_2_StD:  0.010170975371752149\n",
            "Model_2 average cost: 169.22055444717407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:48<00:00, 29.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  179.60268807411194  sec \n",
            " MSE: 1.2764729012840297 ; Model_2_estimate:  0.010707886280449164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:35<00:00, 32.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  166.46436858177185  sec \n",
            " MSE: 1.1534852847825545 ; Model_2_estimate:  -0.002967818356582366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:38<00:00, 31.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  170.21884298324585  sec \n",
            " MSE: 1.3100408757819169 ; Model_2_estimate:  0.006752075064639483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:35<00:00, 32.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  167.48920798301697  sec \n",
            " MSE: 1.2911556714848258 ; Model_2_estimate:  0.017637089785873337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:35<00:00, 32.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  168.58755826950073  sec \n",
            " MSE: 1.2712784022183938 ; Model_2_estimate:  0.03260672985405592\n",
            "iteration:  40 Model_2_MSE:  1.2389241238432813 Model_2_estimate:  0.017919645102072557 Model_2_SE:  0.01059150499152705\n",
            "Model_2_StD:  0.01056783720734049\n",
            "Model_2 average cost: 169.37715929150582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:34<00:00, 32.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  168.41632294654846  sec \n",
            " MSE: 1.243389908404719 ; Model_2_estimate:  0.005939735932951174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:37<00:00, 31.68it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  171.18821120262146  sec \n",
            " MSE: 1.438130750673962 ; Model_2_estimate:  0.04944058282503846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [03:02<00:00, 27.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  195.98943042755127  sec \n",
            " MSE: 1.1429299694678887 ; Model_2_estimate:  0.03354602705046749\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:51<00:00, 29.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  187.45316314697266  sec \n",
            " MSE: 1.2511749943970274 ; Model_2_estimate:  0.03619108992815153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:44<00:00, 30.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  179.66871523857117  sec \n",
            " MSE: 1.3489870152783665 ; Model_2_estimate:  0.009873306250511879\n",
            "iteration:  45 Model_2_MSE:  1.2440350575989605 Model_2_estimate:  0.018928367690444956 Model_2_SE:  0.010616067220952341\n",
            "Model_2_StD:  0.011739051342799789\n",
            "Model_2 average cost: 170.617931260003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:41<00:00, 30.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  177.86871695518494  sec \n",
            " MSE: 1.2294478738883696 ; Model_2_estimate:  0.012747638912317817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:38<00:00, 31.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  175.5955686569214  sec \n",
            " MSE: 1.1451036884952686 ; Model_2_estimate:  0.016682195433998754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:44<00:00, 30.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  180.85665035247803  sec \n",
            " MSE: 1.324056614262477 ; Model_2_estimate:  0.012028211540923804\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [03:17<00:00, 25.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  215.95159339904785  sec \n",
            " MSE: 1.1742045957847167 ; Model_2_estimate:  0.03448277545939132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [02:48<00:00, 29.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  185.44930958747864  sec \n",
            " MSE: 1.220940598113082 ; Model_2_estimate:  0.011584744859672013\n",
            "iteration:  50 Model_2_MSE:  1.2415066192499424 Model_2_estimate:  0.018786042245526536 Model_2_SE:  0.010464795100259662\n",
            "Model_2_StD:  0.011477764689112553\n",
            "Model_2 average cost: 172.27069018363952\n",
            "n: 649  p:  1500  Monte Carlo: 50\n",
            "a0= -1  a1= -3  a2= 3  a3= -1  a4= -3  a5= 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvIg9QbnxQ58"
      },
      "source": [
        "### MAR ###\n",
        "iteration:  50 Model_2_MSE:  1.2678542839526705 Model_2_estimate:  0.01871754528667682 Model_2_SE:  0.010535515787772485\n",
        "Model_2_StD:  0.011843121959576243\n",
        "Model_2 average cost: 187.677102560997\n",
        "n: 649  p:  1000  Monte Carlo: 50\n",
        "a0= -1  a1= -3  a2= 3  a3= -1  a4= -3  a5= 2\n",
        "\n",
        "iteration:  50 Model_2_MSE:  1.2415066192499424 Model_2_estimate:  0.018786042245526536 Model_2_SE:  0.010464795100259662\n",
        "Model_2_StD:  0.011477764689112553\n",
        "Model_2 average cost: 172.27069018363952\n",
        "n: 649  p:  1500  Monte Carlo: 50\n",
        "a0= -1  a1= -3  a2= 3  a3= -1  a4= -3  a5= 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHy6qvxMZbXd"
      },
      "source": [
        "#############  MNAR  ###################\n",
        "iteration:  70 Model_2_MSE:  1.2646627567853355 Model_2_estimate:  0.017924419765486255 Model_2_SE:  0.009299124480622537\n",
        "Model_2_StD:  0.013372046208084708\n",
        "Model_2 average cost: 84.4776154756546\n",
        "n: 649  p:  1000  Monte Carlo: 70\n",
        "a0= -1  a1= -3  a2= 3  a3= -1  a4= -3  a5= 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPz7LzyL1TTX"
      },
      "source": [
        "# ***optimal transport***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ECk6JZPrG0Q",
        "outputId": "b57c63df-f5f7-44ab-9ed8-c997640ab37a"
      },
      "source": [
        "pip install geomloss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geomloss\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/96/97ff3dff46de2c09c7289ef02da574c2b35812a7165edbe1942e2d617bf5/geomloss-0.2.4-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from geomloss) (1.19.5)\n",
            "Installing collected packages: geomloss\n",
            "Successfully installed geomloss-0.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h0ezrowoN5w"
      },
      "source": [
        "from imputers import *\n",
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_estimate=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=-1\n",
        "a1=-3\n",
        "a2=3\n",
        "a3=-1\n",
        "a4=-3\n",
        "a5=2\n",
        "\n",
        "Monte_Carlo =50\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  sample,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,data=adni_data,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5)\n",
        "  # print(sum(missing_row))\n",
        "  truth1=sample[missing_row1==True,:][:,missing_col1==True].copy()\n",
        "  truth2=sample[missing_row2==True,:][:,missing_col2==True].copy()\n",
        "  \n",
        "  s=time.time()\n",
        "  \n",
        "  mis1 = sample[:,missing_col1==True].copy()\n",
        "  mis2 = sample[:,missing_col2==True].copy()\n",
        "  mis1[missing_row1==True,:] = np.nan\n",
        "  mis2[missing_row2==True,:] = np.nan\n",
        "  sample[:,missing_col1==True]=mis1\n",
        "  sample[:,missing_col2==True]=mis2\n",
        "  rawdata = torch.from_numpy(sample)\n",
        "\n",
        "  alist=[]\n",
        "  sk_imputer = OTimputer()\n",
        "  for i in range(1):\n",
        "    sk_imp = sk_imputer.fit_transform(rawdata)\n",
        "    data = sk_imp.detach().cpu().numpy()     \n",
        "    alist.append(data)\n",
        "\n",
        "  mse,b,c,se=data_to_stats(alist,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Estimate: ',b[1])\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[1])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_estimate=(Model_2_estimate*(iteration-1)+b[1])/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_estimate: ',Model_2_estimate,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    # Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    # print('Model_2_SE_median: ',Model_2_SE_median)\n",
        "    print('Model_2 average cost:', Model_2_time)\n",
        "\n",
        "    print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo, 'q:',q)\n",
        "    print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy6mLf7ke8uE"
      },
      "source": [
        "# ***SoftImpute***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUAx7-73W622"
      },
      "source": [
        "from softimpute import *\n",
        "from utils import *\n",
        "\n",
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_estimate=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=-1\n",
        "a1=-3\n",
        "a2=3\n",
        "a3=-1\n",
        "a4=-3\n",
        "a5=2\n",
        "Monte_Carlo = 30\n",
        "\n",
        "grid_len = 5\n",
        "maxit = 500\n",
        "thresh = 1e-4\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  sample,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,data=adni_data,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5)\n",
        "  # print(sum(missing_row))\n",
        "  truth1=sample[missing_row1==True,:][:,missing_col1==True].copy()\n",
        "  truth2=sample[missing_row2==True,:][:,missing_col2==True].copy()\n",
        "  \n",
        "  \n",
        "  mis1 = sample[:,missing_col1==True].copy()\n",
        "  mis2 = sample[:,missing_col2==True].copy()\n",
        "  mis1[missing_row1==True,:] = np.nan\n",
        "  mis2[missing_row2==True,:] = np.nan\n",
        "  sample[:,missing_col1==True]=mis1\n",
        "  sample[:,missing_col2==True]=mis2\n",
        "  rawdata = sample.copy()\n",
        "  \n",
        "  cv_error, grid_lambda = cv_softimpute(rawdata.copy(), grid_len = grid_len, maxit = 50, thresh=1e-3)\n",
        "  s=time.time()\n",
        "  U_thresh, imp = softimpute(rawdata.copy(), grid_lambda[np.argmin(cv_error)], maxit = maxit, thresh=thresh)\n",
        "  alist=[imp]\n",
        "\n",
        "  mse,b,c,se=data_to_stats(alist,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Estimate: ',b[1])\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[1])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_estimate=(Model_2_estimate*(iteration-1)+b[1])/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_estimate: ',Model_2_estimate,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    print('Model_2_SE_median: ',Model_2_SE_median)\n",
        "    print('Model_2 average cost:', Model_2_time)\n",
        "\n",
        "print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo)\n",
        "print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5)\n",
        "print('grid_len=',grid_len, 'maxit=', maxit, 'thresh=',thresh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDARTgtJ2uEb"
      },
      "source": [
        "# **MIGAN2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN-QpqLsd3qH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgcuWWL2h25m"
      },
      "source": [
        "class Generate_data_model(Dataset):   \n",
        "  def __init__(self, array, transform=None):  \n",
        "    self.array = array\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.array)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.array[index]\n",
        "\n",
        "class Generate_data_model_2(Dataset):   \n",
        "  def __init__(self, array, mask, transform=None):  \n",
        "    self.array = array\n",
        "    self.mask = mask             \n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.array)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.array[index], self.mask\n",
        "\n",
        "class Imputer_model_2(nn.Module):\n",
        "  def __init__(self, n1=251, n2=200, n3=100, n4=100):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(n1, n2),\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(n2, n3),\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(n3, n4),\n",
        "    )\n",
        "  def forward(self, data, mask, noise):\n",
        "    net = data * (1 - mask) + noise * mask\n",
        "    net = net.view(data.shape[0], -1)\n",
        "    net = self.fc(net.float())\n",
        "    net = net.view(data.shape[0], -1)\n",
        "    return net\n",
        "\n",
        "class Critic_model_2(nn.Module):\n",
        "  def __init__(self, n1=251, n2=200, n3=50, n4=1):  #n1=251, n2=100, n3=20, n4=1\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(n1, n2),\n",
        "        nn.ReLU(),   \n",
        "        # nn.Tanh(),\n",
        "        nn.Linear(n2, n3),\n",
        "        nn.ReLU(),   \n",
        "        # nn.Tanh(),\n",
        "        nn.Linear(n3, n4),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    out = self.fc(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "class CriticUpdater_model:\n",
        "  def __init__(self, critic, critic_optimizer, batch_size=8, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic = critic\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps = torch.empty(batch_size, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "  def __call__(self, real, fake, device='cuda:0'):\n",
        "    real = real.detach()\n",
        "    fake = fake.detach()\n",
        "    self.critic.zero_grad()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp = (self.eps * real + (1 - self.eps) * fake).to(device).requires_grad_()\n",
        "    grad_d = grad(outputs=self.critic(interp.float()), inputs=interp, grad_outputs=self.ones,create_graph=True)[0]  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d = grad_d.view(real.shape[0], -1)\n",
        "    grad_penalty = ((grad_d.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist = self.critic(fake.float()).mean() - self.critic(real.float()).mean()\n",
        "    loss = w_dist + grad_penalty\n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()\n",
        "\n",
        "class CriticUpdater_model_2:\n",
        "  def __init__(self, critic, critic_optimizer, batch_size=8, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic = critic\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps = torch.empty(batch_size, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "  def __call__(self, real, fake1, fake2, fake3, device='cuda:0'):\n",
        "    real = real.detach()\n",
        "    fake1 = fake1.detach()\n",
        "    fake2 = fake2.detach()\n",
        "    fake3 = fake3.detach()\n",
        "    self.critic.zero_grad()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp1 = (self.eps * real + (1 - self.eps) * fake1).to(device).requires_grad_()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp2 = (self.eps * real + (1 - self.eps) * fake2).to(device).requires_grad_()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp3 = (self.eps * real + (1 - self.eps) * fake3).to(device).requires_grad_()\n",
        "    grad_d1 = grad(outputs=self.critic(interp1.float()), inputs=interp1, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d2 = grad(outputs=self.critic(interp2.float()), inputs=interp2, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)\n",
        "    grad_d3 = grad(outputs=self.critic(interp3.float()), inputs=interp3, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)\n",
        "    grad_penalty1 = ((grad_d1.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty2 = ((grad_d2.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty3 = ((grad_d3.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist1 = self.critic(fake1.float()).mean() - self.critic(real.float()).mean()\n",
        "    w_dist2 = self.critic(fake2.float()).mean() - self.critic(real.float()).mean()\n",
        "    w_dist3 = self.critic(fake3.float()).mean() - self.critic(real.float()).mean()\n",
        "    loss = w_dist1 + w_dist2 + w_dist3 + grad_penalty1 + grad_penalty2 + grad_penalty3 \n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()\n",
        "\n",
        "def mlp_MIGAN2(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI_seed=1):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "\n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  alist = []\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      torch.manual_seed(MI_seed)\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IILfmSk3hWk"
      },
      "source": [
        "## iterative approach, column pattern "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mr1q6bPzLxL"
      },
      "source": [
        "def mlp_MIGAN2_iterative(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "  \n",
        "  alist = []\n",
        "  for j in range(1+MI):\n",
        "    rawdata_pat1 = rawdata[missing_row1, :].copy()\n",
        "    train_pat1 = rawdata[np.logical_not(missing_row1), :].copy()\n",
        "    train_pat1 =  Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    \n",
        "    imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "    imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    \n",
        "    critic_updates = 0\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat1_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "        mask1 = mask1[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat1)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "          cc_hat1 = cc_1.clone()\n",
        "          cc_hat1[:,missing_col1] = imputed_data1\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "          loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "          imputer1.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(missing_row1), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "        rawdata[missing_row1] = rawdata_pat1.copy()\n",
        "\n",
        "    rawdata_pat2 = rawdata[missing_row2, :].copy()\n",
        "    train_pat2 = rawdata[np.logical_not(missing_row2), :].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    \n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat2_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "        mask2 = mask2[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat2)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "          cc_hat2 = cc_2.clone()\n",
        "          cc_hat2[:,missing_col2] = imputed_data2\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "          loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "          imputer2.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer2.eval()\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(missing_row2), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "        rawdata[missing_row2] = rawdata_pat2.copy()\n",
        "\n",
        "    if j > 0:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5iXnyJ8Dp-X"
      },
      "source": [
        "def mlp_MIGAN2_iterative5(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_pat1 = rawdata[missing_row1, :].copy()\n",
        "  rawdata_pat2 = rawdata[missing_row2, :].copy()\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device) \n",
        "  critic1 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device) \n",
        "  critic2 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  \n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  n_critic = 5\n",
        "  alist = []\n",
        "  for j in range(3+MI):\n",
        "    if j==0:\n",
        "      train_pat1 = rawdata[np.logical_or(missing_row1,missing_row2)==0, :].copy()\n",
        "      train_pat1 = Generate_data_model_2(train_pat1, missing_col1)\n",
        "      train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "      train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    if j>0:\n",
        "      train_pat1 = rawdata[np.logical_not(missing_row1), :].copy()\n",
        "      train_pat1 =  Generate_data_model_2(train_pat1, missing_col1)\n",
        "      train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "      train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    \n",
        "    imputer_optimizer1 = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer1 = optim.Adam(critic1.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1_1 = optim.lr_scheduler.StepLR(imputer_optimizer1, step_size=10, gamma=gamma)\n",
        "    scheduler1_2 = optim.lr_scheduler.StepLR(critic_optimizer1, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "    imputer1.train()\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat1_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "        mask1 = mask1[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        update_critic = CriticUpdater_model(critic1, critic_optimizer1, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat1)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "          cc_hat1 = cc_1.clone()\n",
        "          cc_hat1[:,missing_col1] = imputed_data1\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "          loss = -critic1(cc_hat1).mean() + weight*reconstruction_loss\n",
        "          imputer1.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer1.step()\n",
        "      scheduler1_1.step()\n",
        "      scheduler1_2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(missing_row1), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "        rawdata[missing_row1] = rawdata_pat1.copy()\n",
        "    \n",
        "    if j==0:\n",
        "      train_pat2 = rawdata[np.logical_or(missing_row1,missing_row2)==0, :].copy()\n",
        "      train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "      train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "      train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    if j>0:\n",
        "      train_pat2 = rawdata[np.logical_not(missing_row2), :].copy()\n",
        "      train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "      train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "      train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    \n",
        "    imputer_optimizer2 = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer2 = optim.Adam(critic2.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler2_1 = optim.lr_scheduler.StepLR(imputer_optimizer2, step_size=10, gamma=gamma)\n",
        "    scheduler2_2 = optim.lr_scheduler.StepLR(critic_optimizer2, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "    imputer2.train()\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat2_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "        mask2 = mask2[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic2, critic_optimizer2, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat2)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "          cc_hat2 = cc_2.clone()\n",
        "          cc_hat2[:,missing_col2] = imputed_data2\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "          loss = -critic2(cc_hat2).mean() + weight*reconstruction_loss\n",
        "          imputer2.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer2.step()\n",
        "      scheduler2_1.step()\n",
        "      scheduler2_2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer2.eval()\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(missing_row2), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "        rawdata[missing_row2] = rawdata_pat2.copy()\n",
        "\n",
        "    if j > 2:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TeTG5jX3qor"
      },
      "source": [
        "## iterative approach, row pattern, 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cjURFCGcqWj"
      },
      "source": [
        "def mlp_MIGAN2_iterative2(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "  \n",
        "  alist = []\n",
        "  # ilr=0.0002\n",
        "  # clr=0.0002\n",
        "  for j in range(1+MI):\n",
        "    rawdata_pat1 = rawdata[np.logical_and(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat1 = rawdata[np.logical_or(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat1 = Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    \n",
        "    imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "    imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    \n",
        "    critic_updates = 0\n",
        "    for epoch in range(1,1+2*epochs):\n",
        "      for cc, mask in train_pat1_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "        mask1 = mask1[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat1)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "          cc_hat1 = cc_1.clone()\n",
        "          cc_hat1[:,missing_col1] = imputed_data1\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "          loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "          imputer1.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2==False)), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2==False)] = rawdata_pat1.copy()\n",
        "\n",
        "    rawdata_pat2 = rawdata[np.logical_and(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat2 = rawdata[np.logical_or(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    \n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+2*epochs):\n",
        "      for cc, mask in train_pat2_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "        mask2 = mask2[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat2)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "          cc_hat2 = cc_2.clone()\n",
        "          cc_hat2[:,missing_col2] = imputed_data2\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "          loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "          imputer2.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer2.eval()\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1==False,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1==False,missing_row2)] = rawdata_pat2.copy()\n",
        "    \n",
        "    rawdata_pat3 = rawdata[np.logical_and(missing_row1,missing_row2),:].copy()\n",
        "    train_pat3 = rawdata[np.logical_or(missing_row1==False,missing_row2==False),:].copy()\n",
        "    train_pat3 =  Generate_data_model_2(train_pat3, np.logical_or(missing_col1,missing_col2))\n",
        "    \n",
        "    train_pat3_loader1 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat3_loader2 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+2*epochs):\n",
        "      for cc, mask in train_pat3_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_3, mask3 = next(iter(train_pat3_loader2))\n",
        "        cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "        mask3 = mask3[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat3)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "          cc_hat3 = cc_3.clone()\n",
        "          cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "          reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "          loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "          imputer3.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer3.eval()\n",
        "      inc_data_pat3 = torch.from_numpy(rawdata_pat3).to(device)\n",
        "      mask3 = torch.from_numpy(np.logical_or(missing_col1,missing_col2)).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2)] = rawdata_pat3.copy()\n",
        "\n",
        "    if j > 0:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_njX_yo31mj"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-i1IkShZKbR"
      },
      "source": [
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_estimate=0\n",
        "Model_2_time=0\n",
        "\n",
        "MI=3\n",
        "batch_size=1024\n",
        "ilr=1e-3 \n",
        "clr=1e-3 \n",
        "gamma=0.9\n",
        "weight=0.1 \n",
        "gp_lambda=10 \n",
        "epochs=300\n",
        "\n",
        "a0=-1\n",
        "a1=-3\n",
        "a2=3\n",
        "a3=-1\n",
        "a4=-3\n",
        "a5=2\n",
        "Monte_Carlo = 100\n",
        "\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  sample,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,data=adni_data,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5)\n",
        "  # print(sum(missing_row))\n",
        "  truth1=sample[missing_row1==True,:][:,missing_col1==True].copy()\n",
        "  truth2=sample[missing_row2==True,:][:,missing_col2==True].copy()\n",
        "  \n",
        "  mis1 = sample[:,missing_col1==True].copy()\n",
        "  mis2 = sample[:,missing_col2==True].copy()\n",
        "  mis1[missing_row1==True,:] = 0\n",
        "  mis2[missing_row2==True,:] = 0\n",
        "  sample[:,missing_col1==True]=mis1\n",
        "  sample[:,missing_col2==True]=mis2\n",
        "  rawdata = sample.copy()\n",
        "\n",
        "  s=time.time()\n",
        "  # alist=[]\n",
        "  # for i in range(MI):\n",
        "  #   data = mlp_MIGAN2(rawdata.copy(),missing_row1,missing_row2,missing_col1,missing_col2,n=n,p_f=p,Batch_size=batch_size,ilr=ilr,clr=clr,gamma=gamma,weight=weight,gp_lambda=gp_lambda,epochs=epochs,MI_seed=MI)[0]\n",
        "  #   alist.append(data)\n",
        "  \n",
        "  alist = mlp_MIGAN2_iterative2(rawdata.copy(),missing_row1,missing_row2,missing_col1,missing_col2,n=n,p_f=p,Batch_size=batch_size,ilr=ilr,clr=clr,gamma=gamma,weight=weight,gp_lambda=gp_lambda,epochs=epochs,MI=MI)\n",
        "  \n",
        "  mse,b,c,se=data_to_stats(alist,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Estimate: ',b[1])\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[1])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_estimate=(Model_2_estimate*(iteration-1)+b[1])/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_estimate: ',Model_2_estimate,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    # Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    # print('Model_2_SE_median: ',Model_2_SE_median)\n",
        "    print('Model_2 average cost:', Model_2_time/MI)\n",
        "\n",
        "    print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo, 'MI:',MI,'batch_size:',batch_size,'ilr:',ilr,'clr:',clr,'gamma:',gamma,'weight:',weight,'gp_lambda:',gp_lambda,'epochs:',epochs)\n",
        "    print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5, 'q=',q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7KQmUd9euOR"
      },
      "source": [
        "# MIGAN1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLKvUwkDM-5_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzKHzDwmMvGr"
      },
      "source": [
        "class Generate_data_model(Dataset):   \n",
        "  def __init__(self, array, transform=None):  \n",
        "    self.array = array\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.array)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.array[index]\n",
        "\n",
        "class Generate_data_model_2(Dataset):   \n",
        "  def __init__(self, array, mask, transform=None):  \n",
        "    self.array = array\n",
        "    self.mask = mask             \n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.array)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.array[index], self.mask\n",
        "\n",
        "class Imputer_model_2(nn.Module):\n",
        "  def __init__(self, n1=251, n2=200, n3=100, n4=100):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(n1, n2),\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(n2, n3),\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(n3, n4),\n",
        "    )\n",
        "  def forward(self, data, mask, noise):\n",
        "    net = data * (1 - mask) + noise * mask\n",
        "    net = net.view(data.shape[0], -1)\n",
        "    net = self.fc(net.float())\n",
        "    net = net.view(data.shape[0], -1)\n",
        "    return net\n",
        "\n",
        "class Critic_model_2(nn.Module):\n",
        "  def __init__(self, n1=251, n2=200, n3=50, n4=1):  #n1=251, n2=100, n3=20, n4=1\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(n1, n2),\n",
        "        nn.ReLU(),   \n",
        "        # nn.Tanh(),\n",
        "        nn.Linear(n2, n3),\n",
        "        nn.ReLU(),   \n",
        "        # nn.Tanh(),\n",
        "        nn.Linear(n3, n4),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    out = self.fc(x)\n",
        "    return out\n",
        "\n",
        "class CriticUpdater_model:\n",
        "  def __init__(self, critic, critic_optimizer, batch_size=8, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic = critic\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps = torch.empty(batch_size, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "  def __call__(self, real, fake, device='cuda:0'):\n",
        "    real = real.detach()\n",
        "    fake = fake.detach()\n",
        "    self.critic.zero_grad()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp = (self.eps * real + (1 - self.eps) * fake).to(device).requires_grad_()\n",
        "    grad_d = grad(outputs=self.critic(interp.float()), inputs=interp, grad_outputs=self.ones,create_graph=True)[0]  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d = grad_d.view(real.shape[0], -1)\n",
        "    grad_penalty = ((grad_d.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist = self.critic(fake.float()).mean() - self.critic(real.float()).mean()\n",
        "    loss = w_dist + grad_penalty\n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()\n",
        "\n",
        "class CriticUpdater_model_2:\n",
        "  def __init__(self, critic, critic_optimizer, batch_size=8, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic = critic\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps = torch.empty(batch_size, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "  def __call__(self, real, fake1, fake2, fake3, device='cuda:0'):\n",
        "    real = real.detach()\n",
        "    fake1 = fake1.detach()\n",
        "    fake2 = fake2.detach()\n",
        "    fake3 = fake3.detach()\n",
        "    self.critic.zero_grad()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp1 = (self.eps * real + (1 - self.eps) * fake1).to(device).requires_grad_()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp2 = (self.eps * real + (1 - self.eps) * fake2).to(device).requires_grad_()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp3 = (self.eps * real + (1 - self.eps) * fake3).to(device).requires_grad_()\n",
        "    grad_d1 = grad(outputs=self.critic(interp1.float()), inputs=interp1, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d2 = grad(outputs=self.critic(interp2.float()), inputs=interp2, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)\n",
        "    grad_d3 = grad(outputs=self.critic(interp3.float()), inputs=interp3, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)\n",
        "    grad_penalty1 = ((grad_d1.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty2 = ((grad_d2.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty3 = ((grad_d3.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist1 = self.critic(fake1.float()).mean() - self.critic(real.float()).mean()\n",
        "    w_dist2 = self.critic(fake2.float()).mean() - self.critic(real.float()).mean()\n",
        "    w_dist3 = self.critic(fake3.float()).mean() - self.critic(real.float()).mean()\n",
        "    loss = w_dist1 + w_dist2 + w_dist3 + grad_penalty1 + grad_penalty2 + grad_penalty3 \n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()\n",
        "\n",
        "def mlp_MIGAN1(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI_seed=1):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1)\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2)\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3)\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  alist = []\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      torch.manual_seed(MI_seed)\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Ak3n94_sG9"
      },
      "source": [
        "def mlp_MIGAN1_version2(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI_seed=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  \n",
        "  critic_updates = 0\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1.float())\n",
        "        loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "        imputer1.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat2)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2.float())\n",
        "        loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "        imputer2.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3.float())\n",
        "        loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  alist=[]\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      torch.manual_seed(MI_seed)\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQD5Xlsre8SZ"
      },
      "source": [
        "## iterative approach, row pattern, 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv5wiVtkew8A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvg7ob1v3nBs"
      },
      "source": [
        "def mlp_MIGAN1_iterative2(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  \n",
        "  critic_updates = 0\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1.float())\n",
        "        loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "        imputer1.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat2)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2.float())\n",
        "        loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "        imputer2.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3.float())\n",
        "        loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "\n",
        "  \n",
        "  alist = []\n",
        "  for j in range(1+MI):\n",
        "    rawdata_pat1 = rawdata[np.logical_and(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat1 = rawdata[np.logical_or(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat1 = Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    \n",
        "    imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "    imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    \n",
        "    critic_updates = 0\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat1_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "        mask1 = mask1[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat1)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "          cc_hat1 = cc_1.clone()\n",
        "          cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1.float())\n",
        "          loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "          imputer1.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2==False)), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2==False)] = rawdata_pat1.copy()\n",
        "\n",
        "    rawdata_pat2 = rawdata[np.logical_and(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat2 = rawdata[np.logical_or(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    \n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat2_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "        mask2 = mask2[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat2)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "          cc_hat2 = cc_2.clone()\n",
        "          cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2.float())\n",
        "          loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "          imputer2.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer2.eval()\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1==False,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "        rawdata[np.logical_and(missing_row1==False,missing_row2)] = rawdata_pat2.copy()\n",
        "    \n",
        "    rawdata_pat3 = rawdata[np.logical_and(missing_row1,missing_row2),:].copy()\n",
        "    train_pat3 = rawdata[np.logical_or(missing_row1==False,missing_row2==False),:].copy()\n",
        "    train_pat3 =  Generate_data_model_2(train_pat3, np.logical_or(missing_col1,missing_col2))\n",
        "    \n",
        "    train_pat3_loader1 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat3_loader2 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat3_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_3, mask3 = next(iter(train_pat3_loader2))\n",
        "        cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "        mask3 = mask3[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat3)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "          cc_hat3 = cc_3.clone()\n",
        "          cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "          reconstruction_loss += l1_loss(imputed_data3, cc_3.float())\n",
        "          loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "          imputer3.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer3.eval()\n",
        "      inc_data_pat3 = torch.from_numpy(rawdata_pat3).to(device)\n",
        "      mask3 = torch.from_numpy(np.logical_or(missing_col1,missing_col2)).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2)] = rawdata_pat3.copy()\n",
        "\n",
        "    if j > 0:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO9pFGKofBXE"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO0F28cOfKMp"
      },
      "source": [
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_estimate=0\n",
        "Model_2_time=0\n",
        "\n",
        "MI=3\n",
        "batch_size=1024\n",
        "ilr=1e-3 \n",
        "clr=1e-3 \n",
        "gamma=0.9\n",
        "weight=0.1 \n",
        "gp_lambda=10 \n",
        "epochs=300\n",
        "\n",
        "a0=-1\n",
        "a1=-3\n",
        "a2=3\n",
        "a3=-1\n",
        "a4=-3\n",
        "a5=2\n",
        "Monte_Carlo = 100\n",
        "\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  sample,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,data=adni_data,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5)\n",
        "  # print(sum(missing_row))\n",
        "  truth1=sample[missing_row1==True,:][:,missing_col1==True].copy()\n",
        "  truth2=sample[missing_row2==True,:][:,missing_col2==True].copy()\n",
        "  \n",
        "  mis1 = sample[:,missing_col1==True].copy()\n",
        "  mis2 = sample[:,missing_col2==True].copy()\n",
        "  mis1[missing_row1==True,:] = 0\n",
        "  mis2[missing_row2==True,:] = 0\n",
        "  sample[:,missing_col1==True]=mis1\n",
        "  sample[:,missing_col2==True]=mis2\n",
        "  rawdata = sample.copy()\n",
        "\n",
        "  s=time.time()\n",
        "  # alist=[]\n",
        "  # for i in range(MI):\n",
        "  #   data = mlp_MIGAN1_version2(rawdata.copy(),missing_row1,missing_row2,missing_col1,missing_col2,n=n,p_f=p,Batch_size=batch_size,ilr=ilr,clr=clr,gamma=gamma,weight=weight,gp_lambda=gp_lambda,epochs=epochs,MI_seed=MI)[0]\n",
        "  #   alist.append(data)\n",
        "  \n",
        "  alist = mlp_MIGAN1_iterative2(rawdata.copy(),missing_row1,missing_row2,missing_col1,missing_col2,n=n,p_f=p,Batch_size=batch_size,ilr=ilr,clr=clr,gamma=gamma,weight=weight,gp_lambda=gp_lambda,epochs=epochs,MI=MI)\n",
        "  \n",
        "  mse,b,c,se=data_to_stats(alist,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Estimate: ',b[1])\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[1])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_estimate=(Model_2_estimate*(iteration-1)+b[1])/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_estimate: ',Model_2_estimate,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    # Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    # print('Model_2_SE_median: ',Model_2_SE_median)\n",
        "    print('Model_2 average cost:', Model_2_time/MI)\n",
        "\n",
        "    print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo, 'MI:',MI,'batch_size:',batch_size,'ilr:',ilr,'clr:',clr,'gamma:',gamma,'weight:',weight,'gp_lambda:',gp_lambda,'epochs:',epochs)\n",
        "    print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5, 'q=',q)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}