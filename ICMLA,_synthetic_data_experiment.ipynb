{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICMLA, synthetic data experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xcTTQ5zrxHXM",
        "FUXcgg2dyndC",
        "2in8RZShXa9j"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0wdN74lxVW1",
        "outputId": "79d4269e-4dd4-4cc9-8d3c-6edf4989a829"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 12 19:03:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWB4xjN2m4Ff",
        "outputId": "5e2c0ff6-8a6c-4a7a-de6f-f9df615714d5"
      },
      "source": [
        "!pip install missingpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting missingpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/be/998d04d27054b58f0974b5f09f8457778a0a72d4355e0b7ae877b6cfb850/missingpy-0.2.0-py3-none-any.whl (49kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 20kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 30kB 30.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 40kB 24.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hInstalling collected packages: missingpy\n",
            "Successfully installed missingpy-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwnxQ_IlEoce",
        "outputId": "63365907-5d57-455e-f304-4cccf5cad6b1"
      },
      "source": [
        "pip install neural-tangents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neural-tangents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/ee/ee369786e28e7611553722b3abc77cb5d3a071b32ae19c21d155259de60b/neural_tangents-0.3.6-py2.py3-none-any.whl (116kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20kB 28.2MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 30kB 32.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 40kB 24.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 51kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 61kB 13.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 71kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 81kB 13.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 92kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 102kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 112kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax>=0.1.77 in /usr/local/lib/python3.7/dist-packages (from neural-tangents) (0.2.13)\n",
            "Collecting frozendict>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/8c/31de7cd4c8d71825d5e85cef3f0e28afee271345a16fd90b52655f61ab9e/frozendict-2.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.77->neural-tangents) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.77->neural-tangents) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.77->neural-tangents) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.1.77->neural-tangents) (1.15.0)\n",
            "Installing collected packages: frozendict, neural-tangents\n",
            "Successfully installed frozendict-2.0.3 neural-tangents-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEiJrfQ6yPNb",
        "outputId": "588561c6-915f-41d3-d077-e62f9dbc5fc6"
      },
      "source": [
        "# from data_generation import *\n",
        "# from Matrix_Factorization import *\n",
        "# from MICE import *\n",
        "# from gain import *\n",
        "# from misgan import *\n",
        "\n",
        "import copy\n",
        "import statistics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import time\n",
        "import statsmodels.api as sm\n",
        "from missingpy import *\n",
        "from sklearn.linear_model import *\n",
        "from sklearn.svm import *\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.tree import *\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from numpy import linalg as LA\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from matplotlib.patches import Rectangle\n",
        "import pylab as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "from fractions import Fraction \n",
        "\n",
        "import jax.numpy\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import stax\n",
        "from jax import random\n",
        "from jax.experimental import optimizers\n",
        "# from jax.api import jit, grad, vmap\n",
        "import functools\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "n=200;p_f=p=250  #;miss_p=100\n",
        "alpha=0.05\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-bgY4or9tl1"
      },
      "source": [
        "np.random.seed(1)\n",
        "true_beta=np.zeros(p)\n",
        "if p==250:\n",
        "  q=[210,220,230] \n",
        "elif p==50:\n",
        "  q=[40,44,48] \n",
        "elif p==500:\n",
        "  q=[380,400,420]\n",
        "elif p==1000:\n",
        "  q=[650,700,750]    \n",
        "elif p==2000:  \n",
        "  q=[1500,1600,1700]     \n",
        "elif p==1500:\n",
        "  q=[1100,1200,1300]  \n",
        "true_beta[q]=1\n",
        "nonzero_true_beta=true_beta[true_beta!=0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2iHTnwM5hfS"
      },
      "source": [
        "def data_G_linear(n,p,true_beta,miss_pat1=[False,False,False,True,False],miss_pat2=[False,False,False,False,True],a0=0,a1=0.1,a2=0.1,a3=0,a4=0.1,a5=0.1,rho=0.1,seed=1,sigma1=0.2,sigma2=0.5):\n",
        "  # obs is observed data matrix\n",
        "  # mis is truth data matrix but masked\n",
        "  # missing_row is indicator of whether a row has missing values\n",
        "  # missing_col is indicator of whether a col has missing values\n",
        "  # y is label\n",
        "  # true_beta adds sparsity to model, necessary for inference\n",
        "  np.random.seed(seed)\n",
        "  data=np.zeros((n,p))\n",
        "  data[:,0]=np.random.normal(size=(n,),scale=1)            #np.random.laplace   np.random.normal\n",
        "  for col in range(1,p):\n",
        "      data[:,col]=rho*data[:,col-1]+np.random.normal(size=(n,),scale=sigma1)\n",
        "\n",
        "  # split to observed col and missing col\n",
        "  missing_col1=np.array(miss_pat1*int(p/len(miss_pat1)))\n",
        "  missing_col2=np.array(miss_pat2*int(p/len(miss_pat2)))\n",
        "  obs=data[:,np.logical_or(missing_col1,missing_col2)==0]\n",
        "  mis1=data[:,missing_col1==1]\n",
        "  mis2=data[:,missing_col2==1]\n",
        "  y=np.dot(np.concatenate((obs,mis1,mis2),axis=1),true_beta)+np.random.normal(size=(n,),scale=sigma2)\n",
        "  ### MNAR \n",
        "  # logit1=a0+a1*np.mean(mis2,axis=1)+a2*y   # [0:5]\n",
        "  # logit2=a3+a4*np.mean(mis1,axis=1)+a5*y\n",
        "  ### MAR\n",
        "  logit1=a0+a1*(np.mean(obs,axis=1))+a2*y\n",
        "  logit2=a3+a4*(np.mean(obs,axis=1))+a5*y\n",
        "  p_miss1 = 1/(1+np.exp(-logit1))  \n",
        "  p_miss2 = 1/(1+np.exp(-logit2))\n",
        "\n",
        "  missing_row1=np.zeros((n,))\n",
        "  missing_row2=np.zeros((n,))\n",
        "  for i in range(n):\n",
        "    missing_row1[i]= np.random.choice(2, 1, p=[1-p_miss1[i],p_miss1[i]])\n",
        "    missing_row2[i]= np.random.choice(2, 1, p=[1-p_miss2[i],p_miss2[i]])\n",
        "  ### MCAR\n",
        "  # p_miss1 = 0.7\n",
        "  # p_miss2 = 0.6\n",
        "  # for i in range(n):\n",
        "  #   missing_row1[i]= np.random.choice(2, 1, p=[1-p_miss1,p_miss1])\n",
        "  #   missing_row2[i]= np.random.choice(2, 1, p=[1-p_miss2,p_miss2])\n",
        "  miss_col1=np.zeros(p+1)\n",
        "  miss_col1[(p-sum(missing_col1)-sum(missing_col2)):(p-sum(missing_col2))]=1\n",
        "  miss_col2=np.zeros(p+1)\n",
        "  miss_col2[(p-sum(missing_col2)):-1]=1\n",
        " \n",
        "  return obs,mis1,mis2,missing_row1==1,missing_row2==1,miss_col1==1,miss_col2==1,y.reshape((-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFyeywcI7fuu"
      },
      "source": [
        "def data_to_stats(imputed_data_list,true_beta,truth1,truth2,rows1,rows2,cols1,cols2):\n",
        "    rounds=len(imputed_data_list)\n",
        "    valid_p=sum(true_beta!=0)\n",
        "    MSE=0\n",
        "    MEANs=np.zeros((rounds,valid_p))\n",
        "    within_VAR=np.zeros((valid_p,valid_p))\n",
        "    Betw_VAR=np.zeros((valid_p,valid_p))\n",
        "\n",
        "    for i in range(rounds):\n",
        "      imputed_data_MICE=imputed_data_list[i]\n",
        "      X=imputed_data_MICE[:,:-1]\n",
        "      ols = sm.OLS(imputed_data_MICE[:,-1], X[:,true_beta!=0])\n",
        "      result=ols.fit()\n",
        "      within_VAR+=result.cov_params()/rounds\n",
        "      MEANs[i,:]=result.params\n",
        "      # print(imputed_data_MICE.shape)\n",
        "      # print(imputed_data_MICE[:,cols1].shape)\n",
        "      MSE+=np.sum((imputed_data_MICE[rows1,:][:,cols1]-truth1)**2)/(rounds*(np.sum(rows1)*np.sum(cols1)+np.sum(rows2)*np.sum(cols2)))\n",
        "      MSE+=np.sum((imputed_data_MICE[rows2,:][:,cols2]-truth2)**2)/(rounds*(np.sum(rows1)*np.sum(cols1)+np.sum(rows2)*np.sum(cols2)))\n",
        "\n",
        "    if rounds==1:\n",
        "      VAR=within_VAR\n",
        "      MEAN=MEANs[0,:]\n",
        "    else:\n",
        "      # Rubin's rule for total variance\n",
        "      MEAN=np.mean(MEANs,axis=0)\n",
        "      # between imputation variance\n",
        "      for i in range(rounds):\n",
        "          temp=MEANs[i,:]-MEAN\n",
        "          Betw_VAR+=np.outer(temp,temp)/(rounds-1)\n",
        "      VAR=within_VAR+(1+1/rounds)*Betw_VAR\n",
        "    return MSE,MEAN,VAR,np.sqrt(VAR[0][0])\n",
        "\n",
        "def Performance(MEAN,VAR,true_beta,alpha=0.05):\n",
        "    nonzero_true_beta=true_beta[true_beta!=0]\n",
        "    SE=np.sqrt(np.diagonal(VAR))\n",
        "    # elementwise confidence interval\n",
        "    conf=scipy.stats.norm.interval(1-alpha, loc=MEAN[0], scale=SE[0])\n",
        "    coverage=((nonzero_true_beta[0]>conf[0])&(nonzero_true_beta[0]<conf[1]))\n",
        "    bias=nonzero_true_beta[0]-MEAN[0]\n",
        "    return bias,coverage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wMlCxodDHDn"
      },
      "source": [
        "# ***Baseline***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFmgPgEJEp6K",
        "outputId": "d2ba538b-4a69-42a5-cfe6-bae925a8956f"
      },
      "source": [
        "super=time.time()\n",
        "full_SD=[]\n",
        "partial_SD=[]\n",
        "naive_imp_SD=[]\n",
        "\n",
        "full_SE=0\n",
        "partial_SE=0\n",
        "naive_imp_SE=0\n",
        "\n",
        "naive_imp_MSE=0\n",
        "\n",
        "full_bias=0\n",
        "partial_bias=0\n",
        "naive_imp_bias=0\n",
        "\n",
        "full_coverage=0\n",
        "partial_coverage=0\n",
        "naive_imp_coverage=0\n",
        "\n",
        "num_of_missing_row = 0\n",
        "num_of_missing_row1 = 0\n",
        "num_of_missing_row2 = 0\n",
        "\n",
        "a0=1\n",
        "a1=-2\n",
        "a2=3\n",
        "a3=0\n",
        "a4=2\n",
        "a5=-2\n",
        "rho=0.95\n",
        "sigma1=0.1\n",
        "sigma2=0.5\n",
        "\n",
        "Monte_Carlo =500\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  obs,mis1,mis2,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,true_beta=true_beta,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5,rho=rho,sigma1=sigma1,sigma2=sigma2)\n",
        "  # print(sum(missing_row))\n",
        "  truth1=mis1[missing_row1==True,:]\n",
        "  truth2=mis2[missing_row2==True,:]\n",
        "\n",
        "  full=np.concatenate((obs,mis1,mis2,y),axis=1)\n",
        "  partial=full[np.logical_or(missing_row1,missing_row2)==0,:].copy()\n",
        "  \n",
        "  total_row = np.sum(np.logical_or(missing_row1, missing_row2))\n",
        "  row1 = np.sum(missing_row1)\n",
        "  row2 = np.sum(missing_row2)\n",
        "  num_of_missing_row1 = (num_of_missing_row1*(iteration-1)+row1)/iteration\n",
        "  num_of_missing_row2 = (num_of_missing_row2*(iteration-1)+row2)/iteration\n",
        "  num_of_missing_row = (num_of_missing_row*(iteration-1)+total_row)/iteration\n",
        "  ########## benchmark: full data\n",
        "  s=time.time()\n",
        "  mse,b,c,se=data_to_stats([full],true_beta,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  \n",
        "  full_SE=(full_SE*(iteration-1)+se)/iteration\n",
        "  full_SD.append(b[0])\n",
        "  full_bias=(full_bias*(iteration-1)+bias)/iteration\n",
        "  full_coverage=(full_coverage*(iteration-1)+cover)/iteration\n",
        "\n",
        "  ########## benchmark: partial\n",
        "  s=time.time()\n",
        "  X=partial[:,:-1]\n",
        "  ols = sm.OLS(partial[:,-1], X[:,true_beta!=0])\n",
        "  result=ols.fit()\n",
        "  c=result.cov_params()\n",
        "  b=result.params\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  \n",
        "  partial_SE=(partial_SE*(iteration-1)+np.sqrt(c[0][0]))/iteration\n",
        "  partial_SD.append(b[0])\n",
        "  partial_bias=(partial_bias*(iteration-1)+bias)/iteration\n",
        "  partial_coverage=(partial_coverage*(iteration-1)+cover)/iteration\n",
        "\n",
        "  ########## benchmark: naive imputation\n",
        "  mis1[missing_row1,:]=np.mean(mis1[missing_row1==0,:], axis=0)\n",
        "  mis2[missing_row2,:]=np.mean(mis2[missing_row2==0,:], axis=0)\n",
        "  naive_impute=np.concatenate((obs,mis1,mis2,y),axis=1)\n",
        "\n",
        "  s=time.time()\n",
        "  mse,b,c,se=data_to_stats([naive_impute],true_beta,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  \n",
        "  naive_imp_SE=(naive_imp_SE*(iteration-1)+se)/iteration\n",
        "  naive_imp_SD.append(b[0])\n",
        "  naive_imp_MSE=(naive_imp_MSE*(iteration-1)+mse)/iteration\n",
        "  naive_imp_bias=(naive_imp_bias*(iteration-1)+bias)/iteration\n",
        "  naive_imp_coverage=(naive_imp_coverage*(iteration-1)+cover)/iteration\n",
        "\n",
        "print('full_MSE: NA ', 'full_bias ', full_bias, 'full_coverage ', full_coverage, 'full_SE: ',full_SE)\n",
        "print('partial_MSE: NA ', 'partial_bias ', partial_bias, 'partial_coverage ', partial_coverage,'partial_SE: ',partial_SE)\n",
        "print('naive_imp_MSE: ', naive_imp_MSE, 'naive_imp_bias ', naive_imp_bias, 'naive_imp_coverage ', naive_imp_coverage,'naive_imp_SE: ',naive_imp_SE)\n",
        "full_StD=np.std(full_SD, dtype=np.float64)\n",
        "partial_StD=np.std(partial_SD, dtype=np.float64)\n",
        "naive_imp_StD=np.std(naive_imp_SD, dtype=np.float64)\n",
        "print('full_StD: ', full_StD)\n",
        "print('partial_StD: ', partial_StD)\n",
        "print('naive_imp_StD: ', naive_imp_StD)\n",
        "\n",
        "# missing rate\n",
        "print('num_of_missing_row: ',num_of_missing_row)\n",
        "print('num_of_missing_row1: ',num_of_missing_row1)\n",
        "print('num_of_missing_row2: ',num_of_missing_row2)\n",
        "print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo)\n",
        "print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5,' rho=',rho,' sigma1=',sigma1,' sigma2=',sigma2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "full_MSE: NA  full_bias  0.005586164390235798 full_coverage  0.934 full_SE:  0.11029213518978898\n",
            "partial_MSE: NA  partial_bias  0.23691621455169543 partial_coverage  0.8599999999999999 partial_SE:  0.33499818586440816\n",
            "naive_imp_MSE:  0.1407233122435843 naive_imp_bias  0.32824648061145134 naive_imp_coverage  0.7279999999999995 naive_imp_SE:  0.220339447967689\n",
            "full_StD:  0.11628026649469159\n",
            "partial_StD:  0.3392245792851725\n",
            "naive_imp_StD:  0.17650471074321245\n",
            "num_of_missing_row:  179.79999999999998\n",
            "num_of_missing_row1:  127.95200000000011\n",
            "num_of_missing_row2:  99.74999999999999\n",
            "n: 200  p:  250  Monte Carlo: 500\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN-ksz-jWW4f"
      },
      "source": [
        "#########    MAR     ###############\n",
        "full_MSE: NA  full_bias  0.0025348257248207157 full_coverage  0.98 full_SE:  0.06051432012968249\n",
        "partial_MSE: NA  partial_bias  0.18698975081304503 partial_coverage  0.79 partial_SE:  0.22982732749304483\n",
        "naive_imp_MSE:  0.47165325803517083 naive_imp_bias  0.5312439428319942 naive_imp_coverage  0.2800000000000001 naive_imp_SE:  0.224208477812642\n",
        "full_StD:  0.05248328969291819\n",
        "partial_StD:  0.24195935511710312\n",
        "naive_imp_StD:  0.17297712182094002\n",
        "num_of_missing_row:  183.54000000000005\n",
        "num_of_missing_row1:  119.38\n",
        "num_of_missing_row2:  100.20999999999997\n",
        "n: 200  p:  50  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "full_MSE: NA  full_bias  -0.007575247672779701 full_coverage  0.94 full_SE:  0.11135577981887032\n",
        "partial_MSE: NA  partial_bias  0.24429613699649244 partial_coverage  0.88 partial_SE:  0.3768127604475162\n",
        "naive_imp_MSE:  0.1357637458435397 naive_imp_bias  0.0504011604760012 naive_imp_coverage  0.95 naive_imp_SE:  0.35734379739440675\n",
        "full_StD:  0.11462733883297557\n",
        "partial_StD:  0.39464189896035534\n",
        "naive_imp_StD:  0.3203143087937309\n",
        "num_of_missing_row:  183.51\n",
        "num_of_missing_row1:  122.12\n",
        "num_of_missing_row2:  100.99\n",
        "n: 200  p:  500  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "full_MSE: NA  full_bias  -0.010255731336393382 full_coverage  0.96 full_SE:  0.11124314688497172\n",
        "partial_MSE: NA  partial_bias  0.3117022720433771 partial_coverage  0.89 partial_SE:  0.44217351404795807\n",
        "naive_imp_MSE:  0.11453041579688575 naive_imp_bias  -0.005373166073109417 naive_imp_coverage  1.0 naive_imp_SE:  0.3515851722649606\n",
        "full_StD:  0.11159661307137475\n",
        "partial_StD:  0.46222097716954874\n",
        "naive_imp_StD:  0.28244059935672416\n",
        "num_of_missing_row:  184.07000000000005\n",
        "num_of_missing_row1:  122.53\n",
        "num_of_missing_row2:  99.79\n",
        "n: 200  p:  1500  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "full_MSE: NA  full_bias  -0.00848520442899759 full_coverage  0.94 full_SE:  0.11063839584248202\n",
        "partial_MSE: NA  partial_bias  0.2896368381691573 partial_coverage  0.85 partial_SE:  0.3867348992395619\n",
        "naive_imp_MSE:  0.11167624768442445 naive_imp_bias  0.048357738044061765 naive_imp_coverage  0.99 naive_imp_SE:  0.3490580771435289\n",
        "full_StD:  0.10835304405249002\n",
        "partial_StD:  0.4528479315825961\n",
        "naive_imp_StD:  0.2928267597923345\n",
        "num_of_missing_row:  183.72999999999996\n",
        "num_of_missing_row1:  121.75\n",
        "num_of_missing_row2:  100.13\n",
        "n: 200  p:  2000  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2kMCWjgt5T3"
      },
      "source": [
        "#########    MCAR     ###############\n",
        "full_MSE: NA  full_bias  0.03501496966150251 full_coverage  0.94 full_SE:  0.1122019739713618\n",
        "partial_MSE: NA  partial_bias  0.05181274769596977 partial_coverage  0.94 partial_SE:  0.3513214580049656\n",
        "naive_imp_MSE:  0.11219441792543149 naive_imp_bias  0.0631547394095697 naive_imp_coverage  0.99 naive_imp_SE:  0.2770932725046795\n",
        "full_StD:  0.11730948359519462\n",
        "partial_StD:  0.4021832251760242\n",
        "naive_imp_StD:  0.21009377262087947\n",
        "num_of_missing_row:  174.78\n",
        "num_of_missing_row1:  138.12999999999988\n",
        "num_of_missing_row2:  119.29\n",
        "n: 200  p:  1000  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 2  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvPZRZkwUwK9"
      },
      "source": [
        "full_MSE: NA  full_bias  0.03501496966150251 full_coverage  0.94 full_SE:  0.11220197397136182\n",
        "partial_MSE: NA  partial_bias  0.20442581162484252 partial_coverage  0.86 partial_SE:  0.4037255022680312\n",
        "naive_imp_MSE:  0.11694542407684264 naive_imp_bias  0.5131150279866664 naive_imp_coverage  0.51 naive_imp_SE:  0.2624244620052552\n",
        "full_StD:  0.11730948359519462\n",
        "partial_StD:  0.380017129958213\n",
        "naive_imp_StD:  0.21494979685921692\n",
        "num_of_missing_row:  182.05\n",
        "num_of_missing_row1:  130.45000000000005\n",
        "num_of_missing_row2:  100.26\n",
        "n: 200  p:  1000  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 2  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "#####                   MNAR               ###############\n",
        "full_MSE: NA  full_bias  0.0025348257248207157 full_coverage  0.98 full_SE:  0.06051432012968249\n",
        "partial_MSE: NA  partial_bias  0.21435602516308996 partial_coverage  0.78 partial_SE:  0.23407863496403422\n",
        "naive_imp_MSE:  0.4772731772601524 naive_imp_bias  0.5597627357786974 naive_imp_coverage  0.24000000000000005 naive_imp_SE:  0.2246416126534651\n",
        "full_StD:  0.05248328969291819\n",
        "partial_StD:  0.2401891991411309\n",
        "naive_imp_StD:  0.17203557970217498\n",
        "num_of_missing_row:  183.75000000000003\n",
        "num_of_missing_row1:  119.02\n",
        "num_of_missing_row2:  100.28\n",
        "n: 200  p:  50  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "full_MSE: NA  full_bias  -0.002721816608919814 full_coverage  0.9000000000000001 full_SE:  0.10985858010909645\n",
        "partial_MSE: NA  partial_bias  0.25185782377632454 partial_coverage  0.89 partial_SE:  0.33857990870083077\n",
        "naive_imp_MSE:  0.14142110117599305 naive_imp_bias  0.35395471954971763 naive_imp_coverage  0.7199999999999999 naive_imp_SE:  0.2210570628568485\n",
        "full_StD:  0.11413164193102622\n",
        "partial_StD:  0.3319507401756726\n",
        "naive_imp_StD:  0.17127344412593687\n",
        "num_of_missing_row:  180.27000000000004\n",
        "num_of_missing_row1:  127.76999999999998\n",
        "num_of_missing_row2:  100.01\n",
        "n: 200  p:  250  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "full_MSE: NA  full_bias  0.03501496966150251 full_coverage  0.94 full_SE:  0.11220197397136182\n",
        "partial_MSE: NA  partial_bias  0.2824804830031145 partial_coverage  0.7599999999999999 partial_SE:  0.34479927429380547\n",
        "naive_imp_MSE:  0.11309560977924807 naive_imp_bias  0.7022317573458021 naive_imp_coverage  0.11 naive_imp_SE:  0.25722431667802853\n",
        "full_StD:  0.11730948359519462\n",
        "partial_StD:  0.4201487388582143\n",
        "naive_imp_StD:  0.1941220508957793\n",
        "num_of_missing_row:  179.45\n",
        "num_of_missing_row1:  127.02\n",
        "num_of_missing_row2:  99.86\n",
        "n: 200  p:  1000  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcTTQ5zrxHXM"
      },
      "source": [
        "# ***GAIN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1WNuGAx2xKCk",
        "outputId": "406e277d-faff-4529-dbee-6850e09a223d"
      },
      "source": [
        "from gain import gain\n",
        "from utils import rmse_loss\n",
        "\n",
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_bias=0\n",
        "Model_2_coverage=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=1\n",
        "a1=-2\n",
        "a2=3\n",
        "a3=0\n",
        "a4=2\n",
        "a5=-2\n",
        "rho=0.95\n",
        "sigma1=0.1\n",
        "sigma2=0.5\n",
        "Monte_Carlo =100\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  obs,mis1,mis2,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,true_beta=true_beta,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5,rho=rho,sigma1=sigma1,sigma2=sigma2)\n",
        "  truth1=mis1[missing_row1==True,:].copy()\n",
        "  truth2=mis2[missing_row2==True,:].copy()\n",
        "  \n",
        "  s=time.time()\n",
        "\n",
        "  mis1[missing_row1==True,:]=np.nan\n",
        "  mis2[missing_row2==True,:]=np.nan\n",
        "  rawdata = np.concatenate((obs,mis1,mis2,y),axis=1).copy()\n",
        "  \n",
        "  gain_parameters = {'batch_size': 64,\n",
        "                  'hint_rate': 0.9,\n",
        "                  'alpha': 1.0,\n",
        "                  'iterations': 10000}\n",
        "  imputed_data_x = gain(rawdata.copy(), gain_parameters)\n",
        "\n",
        "  mse,b,c,se=data_to_stats([imputed_data_x],true_beta,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Bias: ',bias,'; Coverage: ',cover)\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[0])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_bias=(Model_2_bias*(iteration-1)+bias)/iteration\n",
        "  Model_2_coverage=(Model_2_coverage*(iteration-1)+cover)/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_bias: ',Model_2_bias,'Model_2_coverage: ',Model_2_coverage,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    print('Model_2 average cost:', Model_2_time)\n",
        "\n",
        "    print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo)\n",
        "    print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5,' rho=',rho,' sigma1=',sigma1,' sigma2=',sigma2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  99.8533787727356  sec \n",
            " MSE: 0.75687349301031 ; Bias:  1.6324934279263068 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:37<00:00, 102.24it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  98.85503458976746  sec \n",
            " MSE: 0.7564085098152966 ; Bias:  1.466665029815354 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  99.39159750938416  sec \n",
            " MSE: 0.7940701028658868 ; Bias:  0.21309291809409103 ; Coverage:  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:37<00:00, 102.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  99.32671856880188  sec \n",
            " MSE: 0.7508391715113047 ; Bias:  -0.13440314756209082 ; Coverage:  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:37<00:00, 102.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  99.6635320186615  sec \n",
            " MSE: 0.7878981873781825 ; Bias:  1.6468871904505553 ; Coverage:  False\n",
            "iteration:  5 Model_2_MSE:  0.7692178929161961 Model_2_bias:  0.9649470837448433 Model_2_coverage:  0.4 Model_2_SE:  0.09369143726701643\n",
            "Model_2_StD:  0.7663206829590398\n",
            "Model_2 average cost: 99.41859693527222\n",
            "n: 200  p:  1500  Monte Carlo: 100\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:37<00:00, 102.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  99.74775123596191  sec \n",
            " MSE: 0.7551717500461197 ; Bias:  1.7673874460699075 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  100.51523971557617  sec \n",
            " MSE: 0.7579059160033288 ; Bias:  1.4920028179451212 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [02:52<00:00, 58.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  174.7885377407074  sec \n",
            " MSE: 0.7763147627264118 ; Bias:  1.7723561714805838 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [03:06<00:00, 53.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  188.98035550117493  sec \n",
            " MSE: 0.7436736214717728 ; Bias:  0.8640235021207305 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [02:20<00:00, 71.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  143.87914443016052  sec \n",
            " MSE: 0.7859499325817814 ; Bias:  -0.030859599177743835 ; Coverage:  True\n",
            "iteration:  10 Model_2_MSE:  0.7665105447410396 Model_2_bias:  1.0689645757162816 Model_2_coverage:  0.3 Model_2_SE:  0.10048730723640369\n",
            "Model_2_StD:  0.7350826344959404\n",
            "Model_2 average cost: 120.50065648555756\n",
            "n: 200  p:  1500  Monte Carlo: 100\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [02:48<00:00, 59.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  171.78375363349915  sec \n",
            " MSE: 0.7878093437548557 ; Bias:  0.3147669475583983 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [02:30<00:00, 66.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  154.60492300987244  sec \n",
            " MSE: 0.736616666049301 ; Bias:  1.6708125430316414 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  102.57467985153198  sec \n",
            " MSE: 0.761298626768821 ; Bias:  1.5564277965309 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:39<00:00, 100.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  102.31461310386658  sec \n",
            " MSE: 0.7560169366325462 ; Bias:  0.4081369794975813 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  101.53064227104187  sec \n",
            " MSE: 0.8074002487029922 ; Bias:  1.7275631163473426 ; Coverage:  False\n",
            "iteration:  15 Model_2_MSE:  0.7676164846212609 Model_2_bias:  1.0911568760085784 Model_2_coverage:  0.2 Model_2_SE:  0.09892281348278664\n",
            "Model_2_StD:  0.7040444068316566\n",
            "Model_2 average cost: 122.52123381296794\n",
            "n: 200  p:  1500  Monte Carlo: 100\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:37<00:00, 102.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  101.31738591194153  sec \n",
            " MSE: 0.7422092715552742 ; Bias:  0.05148907770677269 ; Coverage:  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  102.25117564201355  sec \n",
            " MSE: 0.742872272872303 ; Bias:  0.2382418108673363 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  102.95127749443054  sec \n",
            " MSE: 0.7911203398676481 ; Bias:  0.7265609670510506 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  102.75239586830139  sec \n",
            " MSE: 0.77460566552892 ; Bias:  0.0015891084857698745 ; Coverage:  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:37<00:00, 102.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  101.98301100730896  sec \n",
            " MSE: 0.7080118251027696 ; Bias:  0.40034681334402966 ; Coverage:  False\n",
            "iteration:  20 Model_2_MSE:  0.7636533322122914 Model_2_bias:  0.8892790458791817 Model_2_coverage:  0.25 Model_2_SE:  0.09909089466615269\n",
            "Model_2_StD:  0.7150442159269396\n",
            "Model_2 average cost: 117.45390769243241\n",
            "n: 200  p:  1500  Monte Carlo: 100\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  103.1393096446991  sec \n",
            " MSE: 0.7658049669937388 ; Bias:  1.5050879654861185 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  103.17537951469421  sec \n",
            " MSE: 0.7414539860931548 ; Bias:  1.6886172769850771 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:37<00:00, 102.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  102.48044919967651  sec \n",
            " MSE: 0.7609584164730807 ; Bias:  0.07284641449351192 ; Coverage:  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:37<00:00, 102.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  102.89939284324646  sec \n",
            " MSE: 0.742941715138552 ; Bias:  0.07939549997058815 ; Coverage:  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:41<00:00, 98.24it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  107.14779663085938  sec \n",
            " MSE: 0.7875886676744472 ; Bias:  1.5371552943680737 ; Coverage:  False\n",
            "iteration:  25 Model_2_MSE:  0.7628725758647521 Model_2_bias:  0.9067473347554801 Model_2_coverage:  0.28 Model_2_SE:  0.09400274449945932\n",
            "Model_2_StD:  0.7205141373425118\n",
            "Model_2 average cost: 114.71695168495178\n",
            "n: 200  p:  1500  Monte Carlo: 100\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:37<00:00, 102.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  103.24056506156921  sec \n",
            " MSE: 0.7807894673182486 ; Bias:  0.8958931204405839 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:38<00:00, 101.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  104.01379919052124  sec \n",
            " MSE: 0.7683458834887136 ; Bias:  0.2534075716649413 ; Coverage:  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▎     | 4352/10000 [00:42<00:54, 102.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6ffcdc441ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                   \u001b[0;34m'alpha'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                   'iterations': 10000}\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mimputed_data_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_to_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimputed_data_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_beta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruth1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruth2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmissing_row1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmissing_row2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmissing_col1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmissing_col2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gain.py\u001b[0m in \u001b[0;36mgain\u001b[0;34m(data_x, gain_parameters)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     _, D_loss_curr = sess.run([D_solver, D_loss_temp], \n\u001b[0;32m--> 156\u001b[0;31m                               feed_dict = {M: M_mb, X: X_mb, H: H_mb})\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     sess.run([G_solver, G_loss_temp, MSE_loss],\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1369\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1360\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1451\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6oeuW2x8LYu"
      },
      "source": [
        "##### MAR #####\n",
        "iteration:  100 Model_2_MSE:  1.3560164769618632 Model_2_bias:  0.3213163922483485 Model_2_coverage:  0.38000000000000006 Model_2_SE:  0.11425713536740449\n",
        "Model_2_StD:  0.42629791528092675\n",
        "Model_2 average cost: 34.625168442726135\n",
        "n: 200  p:  50  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "iteration:  100 Model_2_MSE:  0.868519623542013 Model_2_bias:  0.6257938960994572 Model_2_coverage:  0.18000000000000005 Model_2_SE:  0.14630196611986063\n",
        "Model_2_StD:  0.5424637137478175\n",
        "Model_2 average cost: 39.06981110811233\n",
        "n: 200  p:  250  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "iteration:  65 Model_2_MSE:  0.7906958188677685 Model_2_bias:  0.6976235556145928 Model_2_coverage:  0.2461538461538462 Model_2_SE:  0.10989737736966337\n",
        "Model_2_StD:  0.7274650834795663\n",
        "Model_2 average cost: 48.58079683597271\n",
        "n: 200  p:  500  Monte Carlo: 100\n",
        "\n",
        "iteration:  25 Model_2_MSE:  0.7628725758647521 Model_2_bias:  0.9067473347554801 Model_2_coverage:  0.28 Model_2_SE:  0.09400274449945932\n",
        "Model_2_StD:  0.7205141373425118\n",
        "Model_2 average cost: 114.71695168495178\n",
        "n: 200  p:  1500  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "iteration:  100 Model_2_MSE:  0.7383403064925876 Model_2_bias:  0.6897764227348189 Model_2_coverage:  0.18000000000000005 Model_2_SE:  0.1697709171016964\n",
        "Model_2_StD:  0.5693423386150692\n",
        "Model_2 average cost: 98.04742572784424\n",
        "n: 200  p:  1000  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23k3wAdxCJjP"
      },
      "source": [
        "iteration:  100 Model_2_MSE:  0.7190594273913655 Model_2_bias:  0.5685083773487243 Model_2_coverage:  0.30000000000000004 Model_2_SE:  0.19027379712303272\n",
        "Model_2_StD:  0.6155049856801302\n",
        "Model_2 average cost: 101.22782071352005\n",
        "n: 200  p:  1000  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 2  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "##### MNAR #####\n",
        "iteration:  100 Model_2_MSE:  1.4822396753689475 Model_2_bias:  0.44481586588300887 Model_2_coverage:  0.24000000000000007 Model_2_SE:  0.11876120953844352\n",
        "Model_2_StD:  0.46417549181255285\n",
        "Model_2 average cost: 35.9048989367485\n",
        "n: 200  p:  50  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "iteration:  100 Model_2_MSE:  0.8618507166789123 Model_2_bias:  0.6212997615311452 Model_2_coverage:  0.18 Model_2_SE:  0.1502809565022466\n",
        "Model_2_StD:  0.5088585380570896\n",
        "Model_2 average cost: 53.24450348854065\n",
        "n: 200  p:  250  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "iteration:  100 Model_2_MSE:  0.7395177331253409 Model_2_bias:  0.6488564603209248 Model_2_coverage:  0.18 Model_2_SE:  0.17198748060019273\n",
        "Model_2_StD:  0.5830330156383707\n",
        "Model_2 average cost: 111.13734125375748\n",
        "n: 200  p:  1000  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUXcgg2dyndC"
      },
      "source": [
        "# ***MICE***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vovtJu2pymy8"
      },
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import *\n",
        "\n",
        "def MICE(missing_data,estimator,seed=1,whether_MI=False):\n",
        "    MICE_imputer=IterativeImputer(estimator=estimator,skip_complete=True,max_iter=5, tol=0.01,sample_posterior=whether_MI,random_state=seed)\n",
        "    imputed_data_MICE=MICE_imputer.fit_transform(missing_data.copy())\n",
        "    \n",
        "    return imputed_data_MICE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41hErWTPC0OL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c695bf1-b8cb-45ff-b445-5c56ec05338e"
      },
      "source": [
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_bias=0\n",
        "Model_2_coverage=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=1\n",
        "a1=-2\n",
        "a2=3\n",
        "a3=0\n",
        "a4=2\n",
        "a5=-2\n",
        "rho=0.95\n",
        "sigma1=0.1\n",
        "sigma2=0.5\n",
        "Monte_Carlo =100\n",
        "MI =3\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  obs,mis1,mis2,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,true_beta=true_beta,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5,rho=rho,sigma1=sigma1,sigma2=sigma2)\n",
        "  truth1=mis1[missing_row1==True,:].copy()\n",
        "  truth2=mis2[missing_row2==True,:].copy()\n",
        "  \n",
        "  s=time.time()\n",
        "\n",
        "  mis1[missing_row1==True,:]=np.nan\n",
        "  mis2[missing_row2==True,:]=np.nan\n",
        "  rawdata = np.concatenate((obs,mis1,mis2,y),axis=1).copy()\n",
        "  \n",
        "  alist=[]\n",
        "  for i in range(MI):\n",
        "    data=MICE(rawdata.copy(),BayesianRidge(),seed=i,whether_MI=True)       \n",
        "    alist.append(data)\n",
        "\n",
        "  mse,b,c,se=data_to_stats(alist,true_beta,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Bias: ',bias,'; Coverage: ',cover)\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[0])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_bias=(Model_2_bias*(iteration-1)+bias)/iteration\n",
        "  Model_2_coverage=(Model_2_coverage*(iteration-1)+cover)/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_bias: ',Model_2_bias,'Model_2_coverage: ',Model_2_coverage,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    # Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    print('Model_2 average cost:', Model_2_time/MI)\n",
        "\n",
        "    print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo,'MI:',MI)\n",
        "    print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5,' rho=',rho,' sigma1=',sigma1,' sigma2=',sigma2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model_2 cost  97.95703220367432  sec \n",
            " MSE: 0.026394539269082237 ; Bias:  0.09128316607199938 ; Coverage:  True\n",
            "Model_2 cost  95.49221706390381  sec \n",
            " MSE: 0.0260120778959902 ; Bias:  0.10381988715311674 ; Coverage:  True\n",
            "Model_2 cost  91.7993655204773  sec \n",
            " MSE: 0.025409298758667657 ; Bias:  0.0853480257624768 ; Coverage:  True\n",
            "Model_2 cost  93.03799295425415  sec \n",
            " MSE: 0.026022775835384496 ; Bias:  0.020560371348346762 ; Coverage:  True\n",
            "Model_2 cost  99.16528058052063  sec \n",
            " MSE: 0.02706865613050196 ; Bias:  -0.049075044023000336 ; Coverage:  True\n",
            "iteration:  5 Model_2_MSE:  0.02618146957792531 Model_2_bias:  0.05038728126258787 Model_2_coverage:  1.0 Model_2_SE:  0.10724037887729307\n",
            "Model_2_StD:  0.05750159846270739\n",
            "Model_2 average cost: 31.83052398363749\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  80.5021424293518  sec \n",
            " MSE: 0.027497918819246996 ; Bias:  -0.06066572137967596 ; Coverage:  True\n",
            "Model_2 cost  92.32874655723572  sec \n",
            " MSE: 0.02605715702209289 ; Bias:  -0.06446534470471454 ; Coverage:  True\n",
            "Model_2 cost  89.72252702713013  sec \n",
            " MSE: 0.027139019925443687 ; Bias:  -0.07211309219161688 ; Coverage:  True\n",
            "Model_2 cost  99.20244812965393  sec \n",
            " MSE: 0.02684331880123822 ; Bias:  -0.2749289154158403 ; Coverage:  False\n",
            "Model_2 cost  101.15372037887573  sec \n",
            " MSE: 0.02602977006737605 ; Bias:  0.04442716477163644 ; Coverage:  True\n",
            "iteration:  10 Model_2_MSE:  0.026447453252502435 Model_2_bias:  -0.01758095026072719 Model_2_coverage:  0.9 Model_2_SE:  0.11151608977325489\n",
            "Model_2_StD:  0.10804246922323342\n",
            "Model_2 average cost: 31.345711922645567\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  102.48716187477112  sec \n",
            " MSE: 0.025460200005456533 ; Bias:  -0.06310704908595954 ; Coverage:  True\n",
            "Model_2 cost  96.60252451896667  sec \n",
            " MSE: 0.026430391023467337 ; Bias:  0.01147084655376418 ; Coverage:  True\n",
            "Model_2 cost  93.98481512069702  sec \n",
            " MSE: 0.02591122139820552 ; Bias:  -0.06392764209637836 ; Coverage:  True\n",
            "Model_2 cost  98.40210461616516  sec \n",
            " MSE: 0.025647247599769805 ; Bias:  -0.05545106786711873 ; Coverage:  True\n",
            "Model_2 cost  94.14949059486389  sec \n",
            " MSE: 0.02592463580899426 ; Bias:  0.06522472948151459 ; Coverage:  True\n",
            "iteration:  15 Model_2_MSE:  0.026256548557394517 Model_2_bias:  -0.018773312374763318 Model_2_coverage:  0.9333333333333333 Model_2_SE:  0.11245057329183193\n",
            "Model_2_StD:  0.09311856258518647\n",
            "Model_2 average cost: 31.688952091005117\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  90.82598280906677  sec \n",
            " MSE: 0.026004420702452614 ; Bias:  0.1420853231007605 ; Coverage:  True\n",
            "Model_2 cost  92.79330229759216  sec \n",
            " MSE: 0.025973917369316393 ; Bias:  -0.006165754153556691 ; Coverage:  True\n",
            "Model_2 cost  114.24751114845276  sec \n",
            " MSE: 0.026658640818163847 ; Bias:  0.1968120475772709 ; Coverage:  True\n",
            "Model_2 cost  86.78950786590576  sec \n",
            " MSE: 0.026064243722290867 ; Bias:  0.03903328503684256 ; Coverage:  True\n",
            "Model_2 cost  109.25238871574402  sec \n",
            " MSE: 0.02778468051102629 ; Bias:  0.21898127351044272 ; Coverage:  True\n",
            "iteration:  20 Model_2_MSE:  0.026316706574208388 Model_2_bias:  0.01545732447251551 Model_2_coverage:  0.95 Model_2_SE:  0.11286733333600836\n",
            "Model_2_StD:  0.10931871176983235\n",
            "Model_2 average cost: 31.998624622821808\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  88.69463109970093  sec \n",
            " MSE: 0.02585925662293567 ; Bias:  -0.032010258160762195 ; Coverage:  True\n",
            "Model_2 cost  96.24348592758179  sec \n",
            " MSE: 0.02580203761629812 ; Bias:  -0.0980981770497531 ; Coverage:  True\n",
            "Model_2 cost  91.28832459449768  sec \n",
            " MSE: 0.026011241707438276 ; Bias:  0.15844084445468531 ; Coverage:  True\n",
            "Model_2 cost  99.35157561302185  sec \n",
            " MSE: 0.026454990628760954 ; Bias:  0.023032075804463203 ; Coverage:  True\n",
            "Model_2 cost  91.44721412658691  sec \n",
            " MSE: 0.027521133745324527 ; Bias:  -0.03719704787359057 ; Coverage:  True\n",
            "iteration:  25 Model_2_MSE:  0.02631931167219701 Model_2_bias:  0.012932557065014115 Model_2_coverage:  0.96 Model_2_SE:  0.11537482028509935\n",
            "Model_2_StD:  0.10531370731242537\n",
            "Model_2 average cost: 31.825957365036007\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  102.46087551116943  sec \n",
            " MSE: 0.025992643076961874 ; Bias:  -0.11803476274608049 ; Coverage:  True\n",
            "Model_2 cost  101.93042397499084  sec \n",
            " MSE: 0.026448973548615903 ; Bias:  -0.2894763058802263 ; Coverage:  True\n",
            "Model_2 cost  86.77821135520935  sec \n",
            " MSE: 0.025120793261099846 ; Bias:  0.4694085744220944 ; Coverage:  False\n",
            "Model_2 cost  94.40906119346619  sec \n",
            " MSE: 0.025249242245972775 ; Bias:  0.10067673747050543 ; Coverage:  True\n",
            "Model_2 cost  98.380366563797  sec \n",
            " MSE: 0.025828004624349185 ; Bias:  0.06499500561734062 ; Coverage:  True\n",
            "iteration:  30 Model_2_MSE:  0.026220748285397496 Model_2_bias:  0.018362772516966216 Model_2_coverage:  0.9333333333333333 Model_2_SE:  0.11887043269732232\n",
            "Model_2_StD:  0.14186534224566064\n",
            "Model_2 average cost: 31.899021614922418\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  83.34221863746643  sec \n",
            " MSE: 0.02606026231307528 ; Bias:  -0.10354740408088103 ; Coverage:  True\n",
            "Model_2 cost  120.78839087486267  sec \n",
            " MSE: 0.025726270843208117 ; Bias:  0.06889643694612035 ; Coverage:  True\n",
            "Model_2 cost  89.1949725151062  sec \n",
            " MSE: 0.02705712034657332 ; Bias:  -0.180823549002344 ; Coverage:  True\n",
            "Model_2 cost  108.20114207267761  sec \n",
            " MSE: 0.02679223938501893 ; Bias:  -0.06677321578294038 ; Coverage:  True\n",
            "Model_2 cost  100.47137928009033  sec \n",
            " MSE: 0.026211717303291172 ; Bias:  0.08335004769686793 ; Coverage:  True\n",
            "iteration:  35 Model_2_MSE:  0.026242001678659765 Model_2_bias:  0.010056728322451696 Model_2_coverage:  0.9428571428571428 Model_2_SE:  0.11986065150951268\n",
            "Model_2_StD:  0.13834955856989523\n",
            "Model_2 average cost: 32.12301624388922\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  111.67424273490906  sec \n",
            " MSE: 0.02693764777009619 ; Bias:  -0.03451014440857114 ; Coverage:  True\n",
            "Model_2 cost  94.41265535354614  sec \n",
            " MSE: 0.02587531158391202 ; Bias:  0.1898233999303165 ; Coverage:  True\n",
            "Model_2 cost  102.93350267410278  sec \n",
            " MSE: 0.025980300604483735 ; Bias:  0.02262350362529053 ; Coverage:  True\n",
            "Model_2 cost  102.84531545639038  sec \n",
            " MSE: 0.02617536506993432 ; Bias:  -0.25523391865086253 ; Coverage:  False\n",
            "Model_2 cost  103.60317516326904  sec \n",
            " MSE: 0.026512039006936137 ; Bias:  0.1534087514954009 ; Coverage:  True\n",
            "iteration:  40 Model_2_MSE:  0.02624876806971136 Model_2_bias:  0.01070242708193459 Model_2_coverage:  0.925 Model_2_SE:  0.11840264727151666\n",
            "Model_2_StD:  0.14100473853103776\n",
            "Model_2 average cost: 32.403246543804805\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  95.4204089641571  sec \n",
            " MSE: 0.02675266154508698 ; Bias:  -0.0023616243734496667 ; Coverage:  True\n",
            "Model_2 cost  91.76110076904297  sec \n",
            " MSE: 0.026398391945225638 ; Bias:  -0.032089188271476665 ; Coverage:  True\n",
            "Model_2 cost  102.34583234786987  sec \n",
            " MSE: 0.026393842348253975 ; Bias:  0.08801853671490678 ; Coverage:  True\n",
            "Model_2 cost  97.18456602096558  sec \n",
            " MSE: 0.026678320748251282 ; Bias:  0.09069295849834946 ; Coverage:  True\n",
            "Model_2 cost  90.99138331413269  sec \n",
            " MSE: 0.02582776727048501 ; Bias:  -0.08806412435581401 ; Coverage:  True\n",
            "iteration:  45 Model_2_MSE:  0.026266704592127935 Model_2_bias:  0.010762080921997767 Model_2_coverage:  0.9333333333333333 Model_2_SE:  0.1183244937588628\n",
            "Model_2_StD:  0.1349424717578298\n",
            "Model_2 average cost: 32.341442609716346\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  100.96628546714783  sec \n",
            " MSE: 0.02696698380209794 ; Bias:  -0.06737391555308014 ; Coverage:  True\n",
            "Model_2 cost  87.77113914489746  sec \n",
            " MSE: 0.02636520585895552 ; Bias:  -0.026028041395755652 ; Coverage:  True\n",
            "Model_2 cost  104.14828705787659  sec \n",
            " MSE: 0.026139514063563206 ; Bias:  -0.09318252306409147 ; Coverage:  True\n",
            "Model_2 cost  96.20056581497192  sec \n",
            " MSE: 0.025849843190959593 ; Bias:  -0.18001943810756393 ; Coverage:  True\n",
            "Model_2 cost  92.8284387588501  sec \n",
            " MSE: 0.025949947548480266 ; Bias:  0.10761652722452553 ; Coverage:  True\n",
            "iteration:  50 Model_2_MSE:  0.026265464022196268 Model_2_bias:  0.004506125011878677 Model_2_coverage:  0.94 Model_2_SE:  0.1187971949050042\n",
            "Model_2_StD:  0.13277883348505012\n",
            "Model_2 average cost: 32.3201090447108\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  107.90541934967041  sec \n",
            " MSE: 0.024481327053163707 ; Bias:  0.17700496439415814 ; Coverage:  True\n",
            "Model_2 cost  87.55907344818115  sec \n",
            " MSE: 0.0272955116089958 ; Bias:  -0.11068751593420023 ; Coverage:  True\n",
            "Model_2 cost  96.54168391227722  sec \n",
            " MSE: 0.025760953678525125 ; Bias:  -0.23993778129855792 ; Coverage:  True\n",
            "Model_2 cost  86.8947684764862  sec \n",
            " MSE: 0.026774918153048072 ; Bias:  -0.11224857310188252 ; Coverage:  True\n",
            "Model_2 cost  93.91986966133118  sec \n",
            " MSE: 0.025863384237807104 ; Bias:  0.12541476022247278 ; Coverage:  True\n",
            "iteration:  55 Model_2_MSE:  0.026244532651660966 Model_2_bias:  0.001179129179562256 Model_2_coverage:  0.9454545454545454 Model_2_SE:  0.11832399564926666\n",
            "Model_2_StD:  0.13564197171215492\n",
            "Model_2 average cost: 32.24751704822887\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  90.06421709060669  sec \n",
            " MSE: 0.025965649268582643 ; Bias:  0.013530065149379378 ; Coverage:  True\n",
            "Model_2 cost  90.65066528320312  sec \n",
            " MSE: 0.02541822241560502 ; Bias:  -0.3209628735798693 ; Coverage:  False\n",
            "Model_2 cost  89.76168751716614  sec \n",
            " MSE: 0.026018814509779543 ; Bias:  -0.1553341812249538 ; Coverage:  True\n",
            "Model_2 cost  99.7044608592987  sec \n",
            " MSE: 0.026899594200736492 ; Bias:  -0.033962513504928404 ; Coverage:  True\n",
            "Model_2 cost  110.15995001792908  sec \n",
            " MSE: 0.026150649658186254 ; Bias:  -0.19094645557973422 ; Coverage:  True\n",
            "iteration:  60 Model_2_MSE:  0.02623170376490405 Model_2_bias:  -0.010380397564403038 Model_2_coverage:  0.9333333333333333 Model_2_SE:  0.11803793730594973\n",
            "Model_2_StD:  0.1396713520315831\n",
            "Model_2 average cost: 32.22881382571327\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  107.057382106781  sec \n",
            " MSE: 0.025339730387833482 ; Bias:  0.10666646816960823 ; Coverage:  True\n",
            "Model_2 cost  103.58658409118652  sec \n",
            " MSE: 0.02702466736870007 ; Bias:  -0.08794447495928792 ; Coverage:  True\n",
            "Model_2 cost  105.11845350265503  sec \n",
            " MSE: 0.02595200554669911 ; Bias:  -0.2792399584621499 ; Coverage:  False\n",
            "Model_2 cost  92.28933954238892  sec \n",
            " MSE: 0.0256781850458938 ; Bias:  -0.08752689537904002 ; Coverage:  True\n",
            "Model_2 cost  88.58859968185425  sec \n",
            " MSE: 0.026312280819691853 ; Bias:  -0.11177404988262762 ; Coverage:  True\n",
            "iteration:  65 Model_2_MSE:  0.026218601462508637 Model_2_bias:  -0.016656042528887378 Model_2_coverage:  0.9230769230769231 Model_2_SE:  0.11757300046116881\n",
            "Model_2_StD:  0.14011858308314892\n",
            "Model_2 average cost: 32.296563319670845\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  106.35152411460876  sec \n",
            " MSE: 0.026716638947535187 ; Bias:  0.16102079167578098 ; Coverage:  True\n",
            "Model_2 cost  93.43285512924194  sec \n",
            " MSE: 0.026270625955442602 ; Bias:  0.1524522900744557 ; Coverage:  True\n",
            "Model_2 cost  111.72637343406677  sec \n",
            " MSE: 0.02684644914946941 ; Bias:  0.013619216156402714 ; Coverage:  True\n",
            "Model_2 cost  98.80013060569763  sec \n",
            " MSE: 0.02494483119995874 ; Bias:  -0.24533845663090648 ; Coverage:  False\n",
            "Model_2 cost  97.01367950439453  sec \n",
            " MSE: 0.026350074765556127 ; Bias:  -0.14667665867002277 ; Coverage:  True\n",
            "iteration:  70 Model_2_MSE:  0.02621911021544319 Model_2_bias:  -0.016393794025313848 Model_2_coverage:  0.9142857142857141 Model_2_SE:  0.11752352386890669\n",
            "Model_2_StD:  0.1417461814017807\n",
            "Model_2 average cost: 32.405515645799184\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  95.1153039932251  sec \n",
            " MSE: 0.026509916182191565 ; Bias:  0.034116925445659696 ; Coverage:  True\n",
            "Model_2 cost  93.89690399169922  sec \n",
            " MSE: 0.02762758282659416 ; Bias:  0.13959970460068705 ; Coverage:  True\n",
            "Model_2 cost  100.84102606773376  sec \n",
            " MSE: 0.026692257676078034 ; Bias:  0.13467692293610012 ; Coverage:  True\n",
            "Model_2 cost  99.13747596740723  sec \n",
            " MSE: 0.027838213787537175 ; Bias:  0.07719840055200289 ; Coverage:  True\n",
            "Model_2 cost  102.52441215515137  sec \n",
            " MSE: 0.02550714758856726 ; Bias:  0.0008093309127122916 ; Coverage:  True\n",
            "iteration:  75 Model_2_MSE:  0.026260171108559886 Model_2_bias:  -0.010148857297664095 Model_2_coverage:  0.92 Model_2_SE:  0.11773250625246798\n",
            "Model_2_StD:  0.13963174260154712\n",
            "Model_2 average cost: 32.429673732121785\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  92.0579948425293  sec \n",
            " MSE: 0.02597568633933173 ; Bias:  -0.18829079320965492 ; Coverage:  True\n",
            "Model_2 cost  100.10017943382263  sec \n",
            " MSE: 0.027111971405698584 ; Bias:  0.011767344835590166 ; Coverage:  True\n",
            "Model_2 cost  105.46065735816956  sec \n",
            " MSE: 0.02595084141249367 ; Bias:  -0.038364418141453305 ; Coverage:  True\n",
            "Model_2 cost  126.72763109207153  sec \n",
            " MSE: 0.025413642731328734 ; Bias:  -0.07405616086661904 ; Coverage:  True\n",
            "Model_2 cost  99.75208353996277  sec \n",
            " MSE: 0.02628965483739972 ; Bias:  -0.3191082602726205 ; Coverage:  False\n",
            "iteration:  80 Model_2_MSE:  0.026253182873353043 Model_2_bias:  -0.01711520731224456 Model_2_coverage:  0.9125 Model_2_SE:  0.11753668784540054\n",
            "Model_2_StD:  0.14102062820293548\n",
            "Model_2 average cost: 32.586577708522476\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  89.65401434898376  sec \n",
            " MSE: 0.026101356762125646 ; Bias:  -0.059834046620761194 ; Coverage:  True\n",
            "Model_2 cost  104.53794026374817  sec \n",
            " MSE: 0.027407722813631293 ; Bias:  -0.12854242476240163 ; Coverage:  True\n",
            "Model_2 cost  100.84584259986877  sec \n",
            " MSE: 0.02692837468969869 ; Bias:  -0.1575322163254642 ; Coverage:  True\n",
            "Model_2 cost  96.19274282455444  sec \n",
            " MSE: 0.025421019836898866 ; Bias:  0.0724279607768662 ; Coverage:  True\n",
            "Model_2 cost  94.8270492553711  sec \n",
            " MSE: 0.02507911600946979 ; Bias:  -0.030472106025381418 ; Coverage:  True\n",
            "iteration:  85 Model_2_MSE:  0.026249320235059624 Model_2_bias:  -0.019684346093373026 Model_2_coverage:  0.9176470588235294 Model_2_SE:  0.11743561329420384\n",
            "Model_2_StD:  0.1385869280033278\n",
            "Model_2 average cost: 32.57584958543965\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  105.37855315208435  sec \n",
            " MSE: 0.025740666744979136 ; Bias:  -0.2593008497068079 ; Coverage:  False\n",
            "Model_2 cost  97.65304040908813  sec \n",
            " MSE: 0.026453507001536886 ; Bias:  0.11528975774438888 ; Coverage:  True\n",
            "Model_2 cost  96.38966941833496  sec \n",
            " MSE: 0.025782076812577395 ; Bias:  -0.18758129944430268 ; Coverage:  True\n",
            "Model_2 cost  121.6424331665039  sec \n",
            " MSE: 0.02593630870499547 ; Bias:  0.012783006560723398 ; Coverage:  True\n",
            "Model_2 cost  89.77478289604187  sec \n",
            " MSE: 0.025580056318672096 ; Bias:  0.06658367990083447 ; Coverage:  True\n",
            "iteration:  90 Model_2_MSE:  0.026229831506253656 Model_2_bias:  -0.021393279143131907 Model_2_coverage:  0.9111111111111111 Model_2_SE:  0.11754303009375563\n",
            "Model_2_StD:  0.13922785230598533\n",
            "Model_2 average cost: 32.65808317749589\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  107.46951341629028  sec \n",
            " MSE: 0.027073787035169442 ; Bias:  -0.17211050305610676 ; Coverage:  True\n",
            "Model_2 cost  91.87255764007568  sec \n",
            " MSE: 0.026518043595644167 ; Bias:  -0.13248039008089685 ; Coverage:  True\n",
            "Model_2 cost  115.83293628692627  sec \n",
            " MSE: 0.02598725219981535 ; Bias:  0.0917479563153325 ; Coverage:  True\n",
            "Model_2 cost  96.31045126914978  sec \n",
            " MSE: 0.026138943691649354 ; Bias:  -0.15983147578087764 ; Coverage:  True\n",
            "Model_2 cost  87.1962480545044  sec \n",
            " MSE: 0.0271739109705284 ; Bias:  -0.32737299668650466 ; Coverage:  False\n",
            "iteration:  95 Model_2_MSE:  0.026248176558480378 Model_2_bias:  -0.027636237180746575 Model_2_coverage:  0.9052631578947369 Model_2_SE:  0.11780127382138873\n",
            "Model_2_StD:  0.14147860712800153\n",
            "Model_2 average cost: 32.689019077702575\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
            "Model_2 cost  95.22562599182129  sec \n",
            " MSE: 0.025914498543040964 ; Bias:  0.08339023431911807 ; Coverage:  True\n",
            "Model_2 cost  99.5983259677887  sec \n",
            " MSE: 0.02537490758743651 ; Bias:  -0.12929213243193605 ; Coverage:  True\n",
            "Model_2 cost  125.73723578453064  sec \n",
            " MSE: 0.025789935084443112 ; Bias:  -0.17432547768327966 ; Coverage:  True\n",
            "Model_2 cost  98.9010488986969  sec \n",
            " MSE: 0.027888727362489256 ; Bias:  -0.07732478854114078 ; Coverage:  True\n",
            "Model_2 cost  100.65894675254822  sec \n",
            " MSE: 0.024916218285885285 ; Bias:  -0.28683515614395283 ; Coverage:  False\n",
            "iteration:  100 Model_2_MSE:  0.02623461059918931 Model_2_bias:  -0.03209829852652116 Model_2_coverage:  0.9 Model_2_SE:  0.1180274566412788\n",
            "Model_2_StD:  0.14189359430180165\n",
            "Model_2 average cost: 32.78832807620366\n",
            "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
            "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YZ8nlnw-9G0"
      },
      "source": [
        "#################################### MAR #################################\n",
        "iteration:  100 Model_2_MSE:  0.020005109410976482 Model_2_bias:  0.0031974062968884325 Model_2_coverage:  0.97 Model_2_SE:  0.06449844945915807\n",
        "Model_2_StD:  0.05675406648193172\n",
        "Model_2 average cost: 6.427422561645508\n",
        "n: 200  p:  50  Monte Carlo: 100 MI: 3\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "iteration:  100 Model_2_MSE:  0.023410699138156348 Model_2_bias:  -0.0061003231592991645 Model_2_coverage:  0.93 Model_2_SE:  0.11672326714181189\n",
        "Model_2_StD:  0.12130368359144336\n",
        "Model_2 average cost: 111.16540475130081\n",
        "n: 200  p:  250  Monte Carlo: 100 MI: 3\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "iteration:  100 Model_2_MSE:  0.02623461059918931 Model_2_bias:  -0.03209829852652116 Model_2_coverage:  0.9 Model_2_SE:  0.1180274566412788\n",
        "Model_2_StD:  0.14189359430180165\n",
        "Model_2 average cost: 32.78832807620366\n",
        "n: 200  p:  500  Monte Carlo: 100 MI: 3\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "iteration:  100 Model_2_MSE:  0.03595457477262314 Model_2_bias:  0.010856693778450286 Model_2_coverage:  0.8200000000000002 Model_2_SE:  0.13463875470413975\n",
        "Model_2_StD:  0.2047276547957849\n",
        "Model_2 average cost: 125.88171967029571\n",
        "n: 200  p:  1000  Monte Carlo: 100 MI: 1\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNNiVd8dwg09"
      },
      "source": [
        "#################################### MNAR #################################\n",
        "iteration:  100 Model_2_MSE:  0.023393896552039967 Model_2_bias:  -0.004964117330061298 Model_2_coverage:  0.93 Model_2_SE:  0.11603503893377896\n",
        "Model_2_StD:  0.12447567814492734\n",
        "Model_2 average cost: 486.21003947734835\n",
        "n: 200  p:  250  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n",
        "\n",
        "iteration:  100 Model_2_MSE:  0.01988731538371139 Model_2_bias:  0.0036740525194513345 Model_2_coverage:  0.98 Model_2_SE:  0.06364716635011386\n",
        "Model_2_StD:  0.05599138862474952\n",
        "Model_2 average cost: 21.363479385375978\n",
        "n: 200  p:  50  Monte Carlo: 100\n",
        "a0= 1  a1= -2  a2= 3  a3= 0  a4= 2  a5= -2  rho= 0.95  sigma1= 0.1  sigma2= 0.5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw3H6WeZREoa"
      },
      "source": [
        "# ***softimpute***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUAx7-73W622"
      },
      "source": [
        "from softimpute import *\n",
        "from utils import *\n",
        "\n",
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_bias=0\n",
        "Model_2_coverage=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=1\n",
        "a1=-2\n",
        "a2=3\n",
        "a3=0\n",
        "a4=2\n",
        "a5=-2\n",
        "rho=0.95\n",
        "sigma1=0.1\n",
        "sigma2=0.5\n",
        "Monte_Carlo =100\n",
        "\n",
        "grid_len =5\n",
        "maxit = 500\n",
        "thresh = 1e-5\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  obs,mis1,mis2,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,true_beta=true_beta,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5,rho=rho,sigma1=sigma1,sigma2=sigma2)\n",
        "  truth1=mis1[missing_row1==True,:].copy()\n",
        "  truth2=mis2[missing_row2==True,:].copy()\n",
        "\n",
        "  mis1[missing_row1==True,:]=np.nan\n",
        "  mis2[missing_row2==True,:]=np.nan\n",
        "  rawdata = np.concatenate((obs,mis1,mis2,y),axis=1).copy()\n",
        "  \n",
        "  cv_error, grid_lambda = cv_softimpute(rawdata.copy(), grid_len = grid_len, maxit = 50, thresh=1e-3)\n",
        "  s=time.time()\n",
        "  U_thresh, imp = softimpute(rawdata.copy(), grid_lambda[np.argmin(cv_error)], maxit = maxit, thresh=thresh)\n",
        "  alist=[imp]\n",
        "\n",
        "  mse,b,c,se=data_to_stats(alist,true_beta,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Bias: ',bias,'; Coverage: ',cover)\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[0])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_bias=(Model_2_bias*(iteration-1)+bias)/iteration\n",
        "  Model_2_coverage=(Model_2_coverage*(iteration-1)+cover)/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_bias: ',Model_2_bias,'Model_2_coverage: ',Model_2_coverage,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    # Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    print('Model_2 average cost:', Model_2_time)\n",
        "\n",
        "    print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo, 'grid_len:',grid_len,'maxit:',maxit,'thresh:',thresh)\n",
        "    print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5,' rho=',rho,' sigma1=',sigma1,' sigma2=',sigma2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB_xXY03n87c"
      },
      "source": [
        "# ***optimal transport-sinkhorn***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ECk6JZPrG0Q",
        "outputId": "f503f114-14a8-4031-e0db-232e30e933f2"
      },
      "source": [
        "pip install geomloss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geomloss\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/96/97ff3dff46de2c09c7289ef02da574c2b35812a7165edbe1942e2d617bf5/geomloss-0.2.4-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from geomloss) (1.19.5)\n",
            "Installing collected packages: geomloss\n",
            "Successfully installed geomloss-0.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h0ezrowoN5w"
      },
      "source": [
        "from imputers import *\n",
        "from utils import *\n",
        "torch.set_default_tensor_type('torch.DoubleTensor')\n",
        "\n",
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_bias=0\n",
        "Model_2_coverage=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=1\n",
        "a1=-2\n",
        "a2=3\n",
        "a3=0\n",
        "a4=2\n",
        "a5=-2\n",
        "rho=0.95\n",
        "sigma1=0.1\n",
        "sigma2=0.5\n",
        "MI = 3\n",
        "Monte_Carlo =50\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  obs,mis1,mis2,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,true_beta=true_beta,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5,rho=rho,sigma1=sigma1,sigma2=sigma2)\n",
        "  truth1=mis1[missing_row1==True,:].copy()\n",
        "  truth2=mis2[missing_row2==True,:].copy()\n",
        "  \n",
        "  s=time.time()\n",
        "\n",
        "  mis1[missing_row1==True,:]=np.nan\n",
        "  mis2[missing_row2==True,:]=np.nan\n",
        "  rawdata = np.concatenate((obs,mis1,mis2,y),axis=1).copy()\n",
        "  \n",
        "  rawdata = torch.from_numpy(rawdata)\n",
        "  epsilon = pick_epsilon(rawdata)\n",
        "  alist=[]\n",
        "  sk_imputer = OTimputer(eps=epsilon)\n",
        "  for i in range(MI):\n",
        "    sk_imp = sk_imputer.fit_transform(rawdata)\n",
        "    data = sk_imp.detach().cpu().numpy()     \n",
        "    alist.append(data)\n",
        "\n",
        "  mse,b,c,se=data_to_stats(alist,true_beta,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Bias: ',bias,'; Coverage: ',cover)\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[0])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_bias=(Model_2_bias*(iteration-1)+bias)/iteration\n",
        "  Model_2_coverage=(Model_2_coverage*(iteration-1)+cover)/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_bias: ',Model_2_bias,'Model_2_coverage: ',Model_2_coverage,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    # Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    print('Model_2 average cost:', Model_2_time)\n",
        "\n",
        "    print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo, 'MI',MI)\n",
        "    print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5,' rho=',rho,' sigma1=',sigma1,' sigma2=',sigma2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBm_MPGFWRx9"
      },
      "source": [
        "# ***optimal transport-round rubin***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDpt-xF_XZtZ",
        "outputId": "73246dad-d778-4e47-bfed-58796eb12a80"
      },
      "source": [
        "pip install geomloss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geomloss\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/96/97ff3dff46de2c09c7289ef02da574c2b35812a7165edbe1942e2d617bf5/geomloss-0.2.4-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from geomloss) (1.19.5)\n",
            "Installing collected packages: geomloss\n",
            "Successfully installed geomloss-0.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeQH4kRXXc_i"
      },
      "source": [
        "from imputers import *\n",
        "from utils import *\n",
        "torch.set_default_tensor_type('torch.DoubleTensor')\n",
        "\n",
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_bias=0\n",
        "Model_2_coverage=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=1\n",
        "a1=-2\n",
        "a2=3\n",
        "a3=0\n",
        "a4=2\n",
        "a5=-2\n",
        "rho=0.95\n",
        "sigma1=0.1\n",
        "sigma2=0.5\n",
        "Monte_Carlo =20\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  obs,mis1,mis2,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,true_beta=true_beta,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5,rho=rho,sigma1=sigma1,sigma2=sigma2)\n",
        "  truth1=mis1[missing_row1==True,:].copy()\n",
        "  truth2=mis2[missing_row2==True,:].copy()\n",
        "  \n",
        "  s=time.time()\n",
        "\n",
        "  mis1[missing_row1==True,:]=np.nan\n",
        "  mis2[missing_row2==True,:]=np.nan\n",
        "  rawdata = np.concatenate((obs,mis1,mis2,y),axis=1).copy()\n",
        "  rawdata = torch.from_numpy(rawdata)\n",
        "\n",
        "  alist=[]\n",
        "  models = {}\n",
        "  for i in range(p+1):\n",
        "    models[i] = nn.Linear(p, 1)\n",
        "  epsilon = pick_epsilon(rawdata)\n",
        "\n",
        "  for i in range(1):\n",
        "    lin_rr_imputer = RRimputer(models, eps=epsilon)\n",
        "    lin_rr_imp = lin_rr_imputer.fit_transform(rawdata)\n",
        "    data = lin_rr_imp.detach().numpy()     \n",
        "    alist.append(data)\n",
        "\n",
        "  mse,b,c,se=data_to_stats(alist,true_beta,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Bias: ',bias,'; Coverage: ',cover)\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[0])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_bias=(Model_2_bias*(iteration-1)+bias)/iteration\n",
        "  Model_2_coverage=(Model_2_coverage*(iteration-1)+cover)/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_bias: ',Model_2_bias,'Model_2_coverage: ',Model_2_coverage,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    print('Model_2 average cost:', Model_2_time)\n",
        "\n",
        "print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo)\n",
        "print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5,' rho=',rho,' sigma1=',sigma1,' sigma2=',sigma2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVsBbQiPZDo_"
      },
      "source": [
        "# ***MIGAN2***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN-QpqLsd3qH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgcuWWL2h25m"
      },
      "source": [
        "class Generate_data_model(Dataset):   \n",
        "  def __init__(self, array, transform=None):  \n",
        "    self.array = array\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.array)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.array[index]\n",
        "\n",
        "class Generate_data_model_2(Dataset):   \n",
        "  def __init__(self, array, mask, transform=None):  \n",
        "    self.array = array\n",
        "    self.mask = mask             \n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.array)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.array[index], self.mask\n",
        "\n",
        "class Imputer_model_2(nn.Module):\n",
        "  def __init__(self, n1=251, n2=200, n3=100, n4=100):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(n1, n2),\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(n2, n3),\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(n3, n4),\n",
        "    )\n",
        "  def forward(self, data, mask, noise):\n",
        "    net = data * (1 - mask) + noise * mask\n",
        "    net = net.view(data.shape[0], -1)\n",
        "    net = self.fc(net.float())\n",
        "    net = net.view(data.shape[0], -1)\n",
        "    return net\n",
        "\n",
        "class Critic_model_2(nn.Module):\n",
        "  def __init__(self, n1=251, n2=200, n3=50, n4=1):  #n1=251, n2=100, n3=20, n4=1\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(n1, n2),\n",
        "        nn.ReLU(),   \n",
        "        # nn.Tanh(),\n",
        "        nn.Linear(n2, n3),\n",
        "        nn.ReLU(),   \n",
        "        # nn.Tanh(),\n",
        "        nn.Linear(n3, n4),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    out = self.fc(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "class CriticUpdater_model:\n",
        "  def __init__(self, critic, critic_optimizer, batch_size=8, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic = critic\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps = torch.empty(batch_size, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "  def __call__(self, real, fake, device='cuda:0'):\n",
        "    real = real.detach()\n",
        "    fake = fake.detach()\n",
        "    self.critic.zero_grad()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp = (self.eps * real + (1 - self.eps) * fake).to(device).requires_grad_()\n",
        "    grad_d = grad(outputs=self.critic(interp.float()), inputs=interp, grad_outputs=self.ones,create_graph=True)[0]  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d = grad_d.view(real.shape[0], -1)\n",
        "    grad_penalty = ((grad_d.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist = self.critic(fake.float()).mean() - self.critic(real.float()).mean()\n",
        "    loss = w_dist + grad_penalty\n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()\n",
        "\n",
        "class CriticUpdater_model_2:\n",
        "  def __init__(self, critic, critic_optimizer, batch_size=8, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic = critic\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps = torch.empty(batch_size, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "  def __call__(self, real, fake1, fake2, fake3, device='cuda:0'):\n",
        "    real = real.detach()\n",
        "    fake1 = fake1.detach()\n",
        "    fake2 = fake2.detach()\n",
        "    fake3 = fake3.detach()\n",
        "    self.critic.zero_grad()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp1 = (self.eps * real + (1 - self.eps) * fake1).to(device).requires_grad_()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp2 = (self.eps * real + (1 - self.eps) * fake2).to(device).requires_grad_()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp3 = (self.eps * real + (1 - self.eps) * fake3).to(device).requires_grad_()\n",
        "    grad_d1 = grad(outputs=self.critic(interp1.float()), inputs=interp1, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d2 = grad(outputs=self.critic(interp2.float()), inputs=interp2, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)\n",
        "    grad_d3 = grad(outputs=self.critic(interp3.float()), inputs=interp3, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)\n",
        "    grad_penalty1 = ((grad_d1.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty2 = ((grad_d2.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty3 = ((grad_d3.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist1 = self.critic(fake1.float()).mean() - self.critic(real.float()).mean()\n",
        "    w_dist2 = self.critic(fake2.float()).mean() - self.critic(real.float()).mean()\n",
        "    w_dist3 = self.critic(fake3.float()).mean() - self.critic(real.float()).mean()\n",
        "    loss = w_dist1 + w_dist2 + w_dist3 + grad_penalty1 + grad_penalty2 + grad_penalty3 \n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()\n",
        "\n",
        "def mlp_MIGAN2(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI_seed=1):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "\n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  alist = []\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      torch.manual_seed(MI_seed)\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDa0XY1m6bjV"
      },
      "source": [
        "def mlp_MIGAN2_version2(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI_seed=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  \n",
        "  critic_updates = 0\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "        loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "        imputer1.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat2)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "        loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "        imputer2.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "        loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  alist=[]\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      torch.manual_seed(MI_seed)\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "      \n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "besQC7OE6aj-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U8laHhgcNy7"
      },
      "source": [
        "## iterative approach, column pattern "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mr1q6bPzLxL"
      },
      "source": [
        "def mlp_MIGAN2_iterative(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "  \n",
        "  alist = []\n",
        "  for j in range(1+MI):\n",
        "    rawdata_pat1 = rawdata[missing_row1, :].copy()\n",
        "    train_pat1 = rawdata[np.logical_not(missing_row1), :].copy()\n",
        "    train_pat1 =  Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    \n",
        "    imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "    imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    \n",
        "    critic_updates = 0\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat1_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "        mask1 = mask1[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat1)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "          cc_hat1 = cc_1.clone()\n",
        "          cc_hat1[:,missing_col1] = imputed_data1\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "          loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "          imputer1.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(missing_row1), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "        rawdata[missing_row1] = rawdata_pat1.copy()\n",
        "\n",
        "    rawdata_pat2 = rawdata[missing_row2, :].copy()\n",
        "    train_pat2 = rawdata[np.logical_not(missing_row2), :].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    \n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat2_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "        mask2 = mask2[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat2)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "          cc_hat2 = cc_2.clone()\n",
        "          cc_hat2[:,missing_col2] = imputed_data2\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "          loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "          imputer2.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer2.eval()\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(missing_row2), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "        rawdata[missing_row2] = rawdata_pat2.copy()\n",
        "\n",
        "    if j > 0:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH7jWTkgchY6"
      },
      "source": [
        "## iterative approach, row pattern, 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cjURFCGcqWj"
      },
      "source": [
        "def mlp_MIGAN2_iterative2(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  \n",
        "  critic_updates = 0\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "        loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "        imputer1.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat2)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "        loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "        imputer2.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "        loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "  \n",
        "  alist = []\n",
        "  for j in range(1+MI):\n",
        "    rawdata_pat1 = rawdata[np.logical_and(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat1 = rawdata[np.logical_or(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat1 = Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    \n",
        "    imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "    imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    \n",
        "    critic_updates = 0\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat1_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "        mask1 = mask1[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat1)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "          cc_hat1 = cc_1.clone()\n",
        "          cc_hat1[:,missing_col1] = imputed_data1\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "          loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "          imputer1.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2==False)), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2==False)] = rawdata_pat1.copy()\n",
        "\n",
        "    rawdata_pat2 = rawdata[np.logical_and(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat2 = rawdata[np.logical_or(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    \n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat2_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "        mask2 = mask2[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat2)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "          cc_hat2 = cc_2.clone()\n",
        "          cc_hat2[:,missing_col2] = imputed_data2\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "          loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "          imputer2.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer2.eval()\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1==False,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1==False,missing_row2)] = rawdata_pat2.copy()\n",
        "    \n",
        "    rawdata_pat3 = rawdata[np.logical_and(missing_row1,missing_row2),:].copy()\n",
        "    train_pat3 = rawdata[np.logical_or(missing_row1==False,missing_row2==False),:].copy()\n",
        "    train_pat3 =  Generate_data_model_2(train_pat3, np.logical_or(missing_col1,missing_col2))\n",
        "    \n",
        "    train_pat3_loader1 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat3_loader2 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat3_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_3, mask3 = next(iter(train_pat3_loader2))\n",
        "        cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "        mask3 = mask3[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat3)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "          cc_hat3 = cc_3.clone()\n",
        "          cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "          reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "          loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "          imputer3.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer3.eval()\n",
        "      inc_data_pat3 = torch.from_numpy(rawdata_pat3).to(device)\n",
        "      mask3 = torch.from_numpy(np.logical_or(missing_col1,missing_col2)).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2)] = rawdata_pat3.copy()\n",
        "\n",
        "    if j > 0:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gw_aQcp11a4"
      },
      "source": [
        "def mlp_MIGAN2_iterative4(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_pat1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_pat2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_pat3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device) \n",
        "  critic1 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_optimizer1 = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer1 = optim.Adam(critic1.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1_1 = optim.lr_scheduler.StepLR(imputer_optimizer1, step_size=10, gamma=gamma)\n",
        "  scheduler1_2 = optim.lr_scheduler.StepLR(critic_optimizer1, step_size=10, gamma=gamma)\n",
        "\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device) \n",
        "  critic2 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer2 = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer2 = optim.Adam(critic2.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler2_1 = optim.lr_scheduler.StepLR(imputer_optimizer2, step_size=10, gamma=gamma)\n",
        "  scheduler2_2 = optim.lr_scheduler.StepLR(critic_optimizer2, step_size=10, gamma=gamma)\n",
        "  \n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device) \n",
        "  critic3 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer3 = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer3 = optim.Adam(critic3.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler3_1 = optim.lr_scheduler.StepLR(imputer_optimizer3, step_size=10, gamma=gamma)\n",
        "  scheduler3_2 = optim.lr_scheduler.StepLR(critic_optimizer3, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1\n",
        "      update_critic = CriticUpdater_model(critic1, critic_optimizer1, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "        loss = -critic1(cc_hat1).mean() + weight*reconstruction_loss\n",
        "        imputer1.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer1.step()\n",
        "    scheduler1_1.step()\n",
        "    scheduler1_2.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2==False)), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1,missing_row2==False)] = rawdata_pat1.copy()\n",
        "   \n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic2, critic_optimizer2, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat2)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "        loss = -critic2(cc_hat2).mean() + weight*reconstruction_loss\n",
        "        imputer2.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer2.step()\n",
        "    scheduler2_1.step()\n",
        "    scheduler2_2.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imputer2.eval()\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1==False,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1==False,missing_row2)] = rawdata_pat2.copy()\n",
        "  \n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic3, critic_optimizer3, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "        loss = -critic3(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer3.step()\n",
        "    scheduler3_1.step()\n",
        "    scheduler3_2.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imputer3.eval()\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_pat3).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_or(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_pat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1,missing_row2)] = rawdata_pat3.copy()\n",
        "\n",
        "\n",
        "  alist = []\n",
        "  for j in range(1+MI):\n",
        "    train_pat1 = rawdata[np.logical_or(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat1 = Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    imputer_optimizer1 = optim.Adam(imputer1.parameters(), lr=ilr/10, betas=(.5, .9))\n",
        "    critic_optimizer1 = optim.Adam(critic1.parameters(), lr=clr/10, betas=(.5, .9))\n",
        "    scheduler1_1 = optim.lr_scheduler.StepLR(imputer_optimizer1, step_size=10, gamma=gamma)\n",
        "    scheduler1_2 = optim.lr_scheduler.StepLR(critic_optimizer1, step_size=10, gamma=gamma)\n",
        "\n",
        "    critic_updates = 0\n",
        "    imputer1.train()\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat1_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "        mask1 = mask1[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        update_critic = CriticUpdater_model(critic1, critic_optimizer1, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat1)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "          cc_hat1 = cc_1.clone()\n",
        "          cc_hat1[:,missing_col1] = imputed_data1\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "          loss = -critic1(cc_hat1).mean() + weight*reconstruction_loss\n",
        "          imputer1.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer1.step()\n",
        "      scheduler1_1.step()\n",
        "      scheduler1_2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2==False)), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2==False)] = rawdata_pat1.copy()\n",
        "\n",
        "    train_pat2 = rawdata[np.logical_or(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    imputer_optimizer2 = optim.Adam(imputer2.parameters(), lr=ilr/10, betas=(.5, .9))\n",
        "    critic_optimizer2 = optim.Adam(critic2.parameters(), lr=clr/10, betas=(.5, .9))\n",
        "    scheduler2_1 = optim.lr_scheduler.StepLR(imputer_optimizer2, step_size=10, gamma=gamma)\n",
        "    scheduler2_2 = optim.lr_scheduler.StepLR(critic_optimizer2, step_size=10, gamma=gamma)\n",
        "\n",
        "    critic_updates = 0\n",
        "    imputer2.train()\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat2_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "        mask2 = mask2[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic2, critic_optimizer2, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat2)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "          cc_hat2 = cc_2.clone()\n",
        "          cc_hat2[:,missing_col2] = imputed_data2\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "          loss = -critic2(cc_hat2).mean() + weight*reconstruction_loss\n",
        "          imputer2.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer2.step()\n",
        "      scheduler2_1.step()\n",
        "      scheduler2_2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer2.eval()\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1==False,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1==False,missing_row2)] = rawdata_pat2.copy()\n",
        "    \n",
        "    train_pat3 = rawdata[np.logical_or(missing_row1==False,missing_row2==False),:].copy()\n",
        "    train_pat3 =  Generate_data_model_2(train_pat3, np.logical_or(missing_col1,missing_col2))\n",
        "    train_pat3_loader1 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat3_loader2 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    imputer_optimizer3 = optim.Adam(imputer3.parameters(), lr=ilr/10, betas=(.5, .9))\n",
        "    critic_optimizer3 = optim.Adam(critic3.parameters(), lr=clr/10, betas=(.5, .9))\n",
        "    scheduler3_1 = optim.lr_scheduler.StepLR(imputer_optimizer3, step_size=10, gamma=gamma)\n",
        "    scheduler3_2 = optim.lr_scheduler.StepLR(critic_optimizer3, step_size=10, gamma=gamma)\n",
        "\n",
        "    critic_updates = 0\n",
        "    imputer3.train()\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat3_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_3, mask3 = next(iter(train_pat3_loader2))\n",
        "        cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "        mask3 = mask3[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic3, critic_optimizer3, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat3)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "          cc_hat3 = cc_3.clone()\n",
        "          cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "          reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "          loss = -critic3(cc_hat3).mean() + weight*reconstruction_loss\n",
        "          imputer3.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer3.step()\n",
        "      scheduler3_1.step()\n",
        "      scheduler3_2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer3.eval()\n",
        "      inc_data_pat3 = torch.from_numpy(rawdata_pat3).to(device)\n",
        "      mask3 = torch.from_numpy(np.logical_or(missing_col1,missing_col2)).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2)] = rawdata_pat3.copy()\n",
        "\n",
        "    if j > 0:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2in8RZShXa9j"
      },
      "source": [
        "## iterative approach, row pattern, 2, joint training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeeagnDpfL89"
      },
      "source": [
        "class CriticUpdater_model_3:\n",
        "  def __init__(self, critic1, critic2, critic3, critic_optimizer, batch_size1, batch_size2, batch_size3, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic1 = critic1\n",
        "    self.critic2 = critic2\n",
        "    self.critic3 = critic3\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps1 = torch.empty(batch_size1, 1,device=device)\n",
        "    self.eps2 = torch.empty(batch_size2, 1,device=device)\n",
        "    self.eps3 = torch.empty(batch_size3, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones1 = torch.ones(batch_size1, 1).to(device)\n",
        "    self.ones2 = torch.ones(batch_size2, 1).to(device)\n",
        "    self.ones3 = torch.ones(batch_size3, 1).to(device)\n",
        "\n",
        "  def __call__(self, real1, real2, real3, fake1, fake2, fake3, device='cuda:0'):\n",
        "    real1 = real1.detach()\n",
        "    real2 = real2.detach()\n",
        "    real3 = real3.detach()\n",
        "    fake1 = fake1.detach()\n",
        "    fake2 = fake2.detach()\n",
        "    fake3 = fake3.detach()\n",
        "    self.critic1.zero_grad()\n",
        "    self.critic2.zero_grad()\n",
        "    self.critic3.zero_grad()\n",
        "    self.eps1.uniform_(0, 1)\n",
        "    interp1 = (self.eps1 * real1 + (1 - self.eps1) * fake1).to(device).requires_grad_()\n",
        "    self.eps2.uniform_(0, 1)\n",
        "    interp2 = (self.eps2 * real2 + (1 - self.eps2) * fake2).to(device).requires_grad_()\n",
        "    self.eps3.uniform_(0, 1)\n",
        "    interp3 = (self.eps3 * real3 + (1 - self.eps3) * fake3).to(device).requires_grad_()\n",
        "    grad_d1 = grad(outputs=self.critic1(interp1.float()), inputs=interp1, grad_outputs=self.ones1,create_graph=True)[0].view(real1.shape[0], -1)  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d2 = grad(outputs=self.critic2(interp2.float()), inputs=interp2, grad_outputs=self.ones2,create_graph=True)[0].view(real2.shape[0], -1)\n",
        "    grad_d3 = grad(outputs=self.critic3(interp3.float()), inputs=interp3, grad_outputs=self.ones3,create_graph=True)[0].view(real3.shape[0], -1)\n",
        "    grad_penalty1 = ((grad_d1.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty2 = ((grad_d2.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty3 = ((grad_d3.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist1 = self.critic1(fake1.float()).mean() - self.critic1(real1.float()).mean()\n",
        "    w_dist2 = self.critic2(fake2.float()).mean() - self.critic2(real2.float()).mean()\n",
        "    w_dist3 = self.critic3(fake3.float()).mean() - self.critic3(real3.float()).mean()\n",
        "    loss = w_dist1 + w_dist2 + w_dist3 + grad_penalty1 + grad_penalty2 + grad_penalty3 \n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCeHTpaWYKBA"
      },
      "source": [
        "def mlp_MIGAN2_iterative3(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2\n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "  \n",
        "  alist = []\n",
        "  for j in range(2+MI):\n",
        "    rawdata_pat1 = rawdata[np.logical_and(missing_row1,missing_row2==False),:].copy()\n",
        "    rawdata_pat2 = rawdata[np.logical_and(missing_row1==False,missing_row2),:].copy()\n",
        "    rawdata_pat3 = rawdata[np.logical_and(missing_row1,missing_row2),:].copy()\n",
        "    train_pat1 = rawdata[np.logical_or(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat1 = Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat2 = rawdata[np.logical_or(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    train_pat3 = rawdata[np.logical_or(missing_row1==False,missing_row2==False),:].copy()\n",
        "    train_pat3 =  Generate_data_model_2(train_pat3, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat3_loader1 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat3_loader2 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "\n",
        "    imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col1)).to(device) \n",
        "    critic1 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)  \n",
        "    imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(missing_col2)).to(device) \n",
        "    critic2 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=sum(np.logical_or(missing_col1,missing_col2))).to(device) \n",
        "    critic3 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    \n",
        "    imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "    critic_params = list(critic1.parameters()) + list(critic2.parameters()) + list(critic3.parameters())\n",
        "    imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic_params, lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "\n",
        "    critic_updates = 0\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc_1, mask1 in train_pat1_loader1:      \n",
        "        cc_1 = cc_1.to(device).float()                           \n",
        "        batch_size1 = cc_1.shape[0]\n",
        "        impu_noise1 = torch.empty(batch_size1, p_f+1, device=device)\n",
        "      \n",
        "        cc_hat1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_hat1 = cc_hat1[0:batch_size1].to(device).float()\n",
        "        mask1 = mask1[0:batch_size1].to(device).float()\n",
        "        imputed_data1 = imputer1(cc_hat1, mask1, impu_noise1.normal_(mean=0,std=1))\n",
        "        cc_hat1[:,missing_col1] = imputed_data1\n",
        "        \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader1))\n",
        "        batch_size2 = cc_2.shape[0]\n",
        "        impu_noise2 = torch.empty(batch_size2, p_f+1, device=device)\n",
        "        cc_2 = cc_2[0:batch_size2].to(device).float()\n",
        "        cc_hat2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_hat2 = cc_hat2[0:batch_size2].to(device).float()\n",
        "        mask2 = mask2[0:batch_size2].to(device).float()\n",
        "        imputed_data2 = imputer2(cc_hat2, mask2, impu_noise2.normal_(mean=0,std=1))\n",
        "        cc_hat2[:,missing_col2] = imputed_data2\n",
        "        \n",
        "        cc_3, mask3 = next(iter(train_pat3_loader1))\n",
        "        batch_size3 = cc_3.shape[0]\n",
        "        impu_noise3 = torch.empty(batch_size3, p_f+1, device=device)\n",
        "        cc_3 = cc_3[0:batch_size3].to(device).float()\n",
        "        cc_hat3, mask3 = next(iter(train_pat3_loader2))\n",
        "        cc_hat3 = cc_hat3[0:batch_size3].to(device).float()\n",
        "        mask3 = mask3[0:batch_size3].to(device).float()\n",
        "        imputed_data3 = imputer3(cc_hat3, mask3, impu_noise3.normal_(mean=0,std=1))\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "\n",
        "        update_critic = CriticUpdater_model_3(critic1, critic2, critic3, critic_optimizer, batch_size1, batch_size2, batch_size3, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc_1, cc_2, cc_3, cc_hat1, cc_hat2, cc_hat3)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          imputed_data1 = imputer1(cc_hat1, mask1, impu_noise1.normal_(mean=0,std=1))\n",
        "          cc_hat1[:,missing_col1] = imputed_data1\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1[:,missing_col1].float())\n",
        "          \n",
        "          imputed_data2 = imputer2(cc_hat2, mask2, impu_noise2.normal_(mean=0,std=1))\n",
        "          cc_hat2[:,missing_col2] = imputed_data2\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2[:,missing_col2].float())\n",
        "          \n",
        "          imputed_data3 = imputer3(cc_hat3, mask3, impu_noise3.normal_(mean=0,std=1))\n",
        "          cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3\n",
        "          reconstruction_loss += l1_loss(imputed_data3, cc_3[:,np.logical_or(missing_col1,missing_col2)].float())\n",
        "\n",
        "          loss = -critic1(cc_hat1).mean()-critic2(cc_hat2).mean()-critic3(cc_hat3).mean() + weight*reconstruction_loss\n",
        "          imputer_optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      imputer2.eval()\n",
        "      imputer3.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      inc_data_pat3 = torch.from_numpy(rawdata_pat3).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      mask3 = torch.from_numpy(np.logical_or(missing_col1,missing_col2)).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2==False)), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2==False)] = rawdata_pat1.copy()\n",
        "        \n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1==False,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1==False,missing_row2)] = rawdata_pat2.copy()\n",
        "        \n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2)] = rawdata_pat3.copy()\n",
        "\n",
        "    if j > 1:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_TfWerUcYrL"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-i1IkShZKbR"
      },
      "source": [
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_bias=0\n",
        "Model_2_coverage=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=1\n",
        "a1=-2\n",
        "a2=3\n",
        "a3=0\n",
        "a4=2\n",
        "a5=-2\n",
        "rho=0.95\n",
        "sigma1=0.1\n",
        "sigma2=0.5\n",
        "Monte_Carlo =100\n",
        "MI=10\n",
        "batch_size=256\n",
        "ilr=1e-3 \n",
        "clr=1e-3 \n",
        "gamma=0.9\n",
        "weight=0.1 \n",
        "gp_lambda=10 \n",
        "epochs=300\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  obs,mis1,mis2,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,true_beta=true_beta,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5,rho=rho,sigma1=sigma1,sigma2=sigma2)\n",
        "  truth1=mis1[missing_row1==True,:].copy()\n",
        "  truth2=mis2[missing_row2==True,:].copy()\n",
        "\n",
        "  mis1[missing_row1==True,:]=0\n",
        "  mis2[missing_row2==True,:]=0\n",
        "  rawdata = np.concatenate((obs,mis1,mis2,y),axis=1).copy()\n",
        "  \n",
        "  s=time.time()\n",
        "  # alist=[]\n",
        "  # for i in range(MI):\n",
        "  #   data = mlp_MIGAN2_version2(rawdata,missing_row1,missing_row2,missing_col1,missing_col2,n=n,p_f=p,Batch_size=batch_size,ilr=ilr,clr=clr,gamma=gamma,weight=weight,gp_lambda=gp_lambda,epochs=epochs,MI_seed=MI)[0]\n",
        "  #   alist.append(data)\n",
        "  \n",
        "  alist = mlp_MIGAN2_iterative2(rawdata,missing_row1,missing_row2,missing_col1,missing_col2,n=n,p_f=p,Batch_size=batch_size,ilr=ilr,clr=clr,gamma=gamma,weight=weight,gp_lambda=gp_lambda,epochs=epochs,MI=MI)\n",
        "\n",
        "  mse,b,c,se=data_to_stats(alist,true_beta,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Bias: ',bias,'; Coverage: ',cover)\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[0])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_bias=(Model_2_bias*(iteration-1)+bias)/iteration\n",
        "  Model_2_coverage=(Model_2_coverage*(iteration-1)+cover)/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_bias: ',Model_2_bias,'Model_2_coverage: ',Model_2_coverage,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    # Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    print('Model_2 average cost:', Model_2_time/MI)\n",
        "\n",
        "    print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo, 'MI:',MI,'batch_size:',batch_size,'ilr:',ilr,'clr:',clr,'gamma:',gamma,'weight:',weight,'gp_lambda:',gp_lambda,'epochs:',epochs)\n",
        "    print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5,' rho=',rho,' sigma1=',sigma1,' sigma2=',sigma2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0HO7BE9Mr7b"
      },
      "source": [
        "# ***MIGAN1***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLKvUwkDM-5_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzKHzDwmMvGr"
      },
      "source": [
        "class Generate_data_model(Dataset):   \n",
        "  def __init__(self, array, transform=None):  \n",
        "    self.array = array\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.array)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.array[index]\n",
        "\n",
        "class Generate_data_model_2(Dataset):   \n",
        "  def __init__(self, array, mask, transform=None):  \n",
        "    self.array = array\n",
        "    self.mask = mask             \n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.array)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.array[index], self.mask\n",
        "\n",
        "class Imputer_model_2(nn.Module):\n",
        "  def __init__(self, n1=251, n2=200, n3=100, n4=100):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(n1, n2),\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(n2, n3),\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(n3, n4),\n",
        "    )\n",
        "  def forward(self, data, mask, noise):\n",
        "    net = data * (1 - mask) + noise * mask\n",
        "    net = net.view(data.shape[0], -1)\n",
        "    net = self.fc(net.float())\n",
        "    net = net.view(data.shape[0], -1)\n",
        "    return net\n",
        "\n",
        "class Critic_model_2(nn.Module):\n",
        "  def __init__(self, n1=251, n2=200, n3=50, n4=1):  #n1=251, n2=100, n3=20, n4=1\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(n1, n2),\n",
        "        nn.ReLU(),   \n",
        "        # nn.Tanh(),\n",
        "        nn.Linear(n2, n3),\n",
        "        nn.ReLU(),   \n",
        "        # nn.Tanh(),\n",
        "        nn.Linear(n3, n4),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    out = self.fc(x)\n",
        "    return out\n",
        "\n",
        "class CriticUpdater_model:\n",
        "  def __init__(self, critic, critic_optimizer, batch_size=8, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic = critic\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps = torch.empty(batch_size, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "  def __call__(self, real, fake, device='cuda:0'):\n",
        "    real = real.detach()\n",
        "    fake = fake.detach()\n",
        "    self.critic.zero_grad()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp = (self.eps * real + (1 - self.eps) * fake).to(device).requires_grad_()\n",
        "    grad_d = grad(outputs=self.critic(interp.float()), inputs=interp, grad_outputs=self.ones,create_graph=True)[0]  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d = grad_d.view(real.shape[0], -1)\n",
        "    grad_penalty = ((grad_d.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist = self.critic(fake.float()).mean() - self.critic(real.float()).mean()\n",
        "    loss = w_dist + grad_penalty\n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()\n",
        "\n",
        "class CriticUpdater_model_2:\n",
        "  def __init__(self, critic, critic_optimizer, batch_size=8, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic = critic\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps = torch.empty(batch_size, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "  def __call__(self, real, fake1, fake2, fake3, device='cuda:0'):\n",
        "    real = real.detach()\n",
        "    fake1 = fake1.detach()\n",
        "    fake2 = fake2.detach()\n",
        "    fake3 = fake3.detach()\n",
        "    self.critic.zero_grad()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp1 = (self.eps * real + (1 - self.eps) * fake1).to(device).requires_grad_()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp2 = (self.eps * real + (1 - self.eps) * fake2).to(device).requires_grad_()\n",
        "    self.eps.uniform_(0, 1)\n",
        "    interp3 = (self.eps * real + (1 - self.eps) * fake3).to(device).requires_grad_()\n",
        "    grad_d1 = grad(outputs=self.critic(interp1.float()), inputs=interp1, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d2 = grad(outputs=self.critic(interp2.float()), inputs=interp2, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)\n",
        "    grad_d3 = grad(outputs=self.critic(interp3.float()), inputs=interp3, grad_outputs=self.ones,create_graph=True)[0].view(real.shape[0], -1)\n",
        "    grad_penalty1 = ((grad_d1.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty2 = ((grad_d2.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty3 = ((grad_d3.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist1 = self.critic(fake1.float()).mean() - self.critic(real.float()).mean()\n",
        "    w_dist2 = self.critic(fake2.float()).mean() - self.critic(real.float()).mean()\n",
        "    w_dist3 = self.critic(fake3.float()).mean() - self.critic(real.float()).mean()\n",
        "    loss = w_dist1 + w_dist2 + w_dist3 + grad_penalty1 + grad_penalty2 + grad_penalty3 \n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()\n",
        "\n",
        "def mlp_MIGAN1(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI_seed=1):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1)\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2)\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3)\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  alist = []\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      torch.manual_seed(MI_seed)\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      # print(sum(np.isnan(imputed_data.cpu().numpy())))\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1gLY42P_rni"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Ak3n94_sG9"
      },
      "source": [
        "def mlp_MIGAN1_version2(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI_seed=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  \n",
        "  critic_updates = 0\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1.float())\n",
        "        loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "        imputer1.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat2)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2.float())\n",
        "        loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "        imputer2.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3.float())\n",
        "        loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  alist=[]\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      torch.manual_seed(MI_seed)\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPz6vRIdz6cK"
      },
      "source": [
        "## iterative approach, column pattern "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41NBaUT27hyS"
      },
      "source": [
        "def mlp_MIGAN1_iterative(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1)\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2)\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3)\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "\n",
        "  alist = []\n",
        "  for j in range(1+MI):\n",
        "    rawdata_pat1 = rawdata[missing_row1, :].copy()\n",
        "    train_pat1 = rawdata[np.logical_not(missing_row1), :].copy()\n",
        "    train_pat1 =  Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    \n",
        "    imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "    imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    \n",
        "    critic_updates = 0\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat1_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "        mask1 = mask1[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat1)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "          cc_hat1 = cc_1.clone()\n",
        "          cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1.float())\n",
        "          loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "          imputer1.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(missing_row1), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "        rawdata[missing_row1] = rawdata_pat1.copy()\n",
        "\n",
        "    rawdata_pat2 = rawdata[missing_row2, :].copy()\n",
        "    train_pat2 = rawdata[np.logical_not(missing_row2), :].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    \n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat2_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "        mask2 = mask2[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat2)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "          cc_hat2 = cc_2.clone()\n",
        "          cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2.float())\n",
        "          loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "          imputer2.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer2.eval()\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(missing_row2), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "        rawdata[missing_row2] = rawdata_pat2.copy()\n",
        "\n",
        "    if j > 0:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w4nCG0U3g4F"
      },
      "source": [
        "## iterative approach, row pattern, 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvg7ob1v3nBs"
      },
      "source": [
        "def mlp_MIGAN1_iterative2(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  \n",
        "  critic_updates = 0\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1.float())\n",
        "        loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "        imputer1.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat2)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2.float())\n",
        "        loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "        imputer2.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "  imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  critic_updates = 0\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "    \n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "\n",
        "      update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3.float())\n",
        "        loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "\n",
        "  \n",
        "  alist = []\n",
        "  for j in range(1+MI):\n",
        "    rawdata_pat1 = rawdata[np.logical_and(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat1 = rawdata[np.logical_or(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat1 = Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    \n",
        "    imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "    imputer_optimizer = optim.Adam(imputer1.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    \n",
        "    critic_updates = 0\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat1_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "        mask1 = mask1[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat1)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "          cc_hat1 = cc_1.clone()\n",
        "          cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1.float())\n",
        "          loss = -critic(cc_hat1).mean() + weight*reconstruction_loss\n",
        "          imputer1.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2==False)), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2==False)] = rawdata_pat1.copy()\n",
        "\n",
        "    rawdata_pat2 = rawdata[np.logical_and(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat2 = rawdata[np.logical_or(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    \n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer2.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat2_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "        mask2 = mask2[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat2)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "          cc_hat2 = cc_2.clone()\n",
        "          cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2.float())\n",
        "          loss = -critic(cc_hat2).mean() + weight*reconstruction_loss\n",
        "          imputer2.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer2.eval()\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1==False,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "        rawdata[np.logical_and(missing_row1==False,missing_row2)] = rawdata_pat2.copy()\n",
        "    \n",
        "    rawdata_pat3 = rawdata[np.logical_and(missing_row1,missing_row2),:].copy()\n",
        "    train_pat3 = rawdata[np.logical_or(missing_row1==False,missing_row2==False),:].copy()\n",
        "    train_pat3 =  Generate_data_model_2(train_pat3, np.logical_or(missing_col1,missing_col2))\n",
        "    \n",
        "    train_pat3_loader1 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat3_loader2 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer_optimizer = optim.Adam(imputer3.parameters(), lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "    critic_updates = 0\n",
        "\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc, mask in train_pat3_loader1:      \n",
        "        cc = cc.to(device).float()                           \n",
        "        batch_size = cc.shape[0]\n",
        "        impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "      \n",
        "        cc_3, mask3 = next(iter(train_pat3_loader2))\n",
        "        cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "        mask3 = mask3[0:batch_size].to(device).float()\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "\n",
        "        update_critic = CriticUpdater_model(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc, cc_hat3)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "          \n",
        "          impu_noise.normal_(mean=0,std=1)\n",
        "          imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "          cc_hat3 = cc_3.clone()\n",
        "          cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "          reconstruction_loss += l1_loss(imputed_data3, cc_3.float())\n",
        "          loss = -critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "          imputer3.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      imputer3.eval()\n",
        "      inc_data_pat3 = torch.from_numpy(rawdata_pat3).to(device)\n",
        "      mask3 = torch.from_numpy(np.logical_or(missing_col1,missing_col2)).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2)] = rawdata_pat3.copy()\n",
        "\n",
        "    if j > 0:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSR6DPPaRR1X"
      },
      "source": [
        "## iterative approach, row pattern, 2, joint training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03QYixU1RVbQ"
      },
      "source": [
        "class CriticUpdater_model_3:\n",
        "  def __init__(self, critic1, critic2, critic3, critic_optimizer, batch_size1, batch_size2, batch_size3, gp_lambda=10, device='cuda:0'):\n",
        "    self.critic1 = critic1\n",
        "    self.critic2 = critic2\n",
        "    self.critic3 = critic3\n",
        "    self.critic_optimizer = critic_optimizer\n",
        "    self.gp_lambda = gp_lambda\n",
        "    # Interpolation coefficient\n",
        "    self.eps1 = torch.empty(batch_size1, 1,device=device)\n",
        "    self.eps2 = torch.empty(batch_size2, 1,device=device)\n",
        "    self.eps3 = torch.empty(batch_size3, 1,device=device)\n",
        "    # For computing the gradient penalty\n",
        "    self.ones1 = torch.ones(batch_size1, 1).to(device)\n",
        "    self.ones2 = torch.ones(batch_size2, 1).to(device)\n",
        "    self.ones3 = torch.ones(batch_size3, 1).to(device)\n",
        "\n",
        "  def __call__(self, real1, real2, real3, fake1, fake2, fake3, device='cuda:0'):\n",
        "    real1 = real1.detach()\n",
        "    real2 = real2.detach()\n",
        "    real3 = real3.detach()\n",
        "    fake1 = fake1.detach()\n",
        "    fake2 = fake2.detach()\n",
        "    fake3 = fake3.detach()\n",
        "    self.critic1.zero_grad()\n",
        "    self.critic2.zero_grad()\n",
        "    self.critic3.zero_grad()\n",
        "    self.eps1.uniform_(0, 1)\n",
        "    interp1 = (self.eps1 * real1 + (1 - self.eps1) * fake1).to(device).requires_grad_()\n",
        "    self.eps2.uniform_(0, 1)\n",
        "    interp2 = (self.eps2 * real2 + (1 - self.eps2) * fake2).to(device).requires_grad_()\n",
        "    self.eps3.uniform_(0, 1)\n",
        "    interp3 = (self.eps3 * real3 + (1 - self.eps3) * fake3).to(device).requires_grad_()\n",
        "    grad_d1 = grad(outputs=self.critic1(interp1.float()), inputs=interp1, grad_outputs=self.ones1,create_graph=True)[0].view(real1.shape[0], -1)  #grad_outputs=self.ones,create_graph=True\n",
        "    grad_d2 = grad(outputs=self.critic2(interp2.float()), inputs=interp2, grad_outputs=self.ones2,create_graph=True)[0].view(real2.shape[0], -1)\n",
        "    grad_d3 = grad(outputs=self.critic3(interp3.float()), inputs=interp3, grad_outputs=self.ones3,create_graph=True)[0].view(real3.shape[0], -1)\n",
        "    grad_penalty1 = ((grad_d1.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty2 = ((grad_d2.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    grad_penalty3 = ((grad_d3.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "    w_dist1 = self.critic1(fake1.float()).mean() - self.critic1(real1.float()).mean()\n",
        "    w_dist2 = self.critic2(fake2.float()).mean() - self.critic2(real2.float()).mean()\n",
        "    w_dist3 = self.critic3(fake3.float()).mean() - self.critic3(real3.float()).mean()\n",
        "    loss = w_dist1 + w_dist2 + w_dist3 + grad_penalty1 + grad_penalty2 + grad_penalty3 \n",
        "    loss.backward()\n",
        "    self.critic_optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i1qQu-TRYT4"
      },
      "source": [
        "def mlp_MIGAN1_iterative3(missing_data, missing_row1,missing_row2,missing_col1,missing_col2, n=200, p_f=250, device='cuda:0', Batch_size=16, ilr=1e-3, clr=1e-3, gamma=0.2, weight=0.1, gp_lambda=10, epochs=300, MI=10):\n",
        "  rawdata = missing_data.copy()\n",
        "  rawdata_inc1 = rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2)), :].copy()\n",
        "  rawdata_inc2 = rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1)), :].copy()\n",
        "  rawdata_inc3 = rawdata[np.logical_and(missing_row1,missing_row2), :].copy()\n",
        "  cc = torch.from_numpy(rawdata.copy()[np.logical_or(missing_row1,missing_row2)==0, :])\n",
        "\n",
        "  cc_data = Generate_data_model(cc)\n",
        "  cc_data_pat1 =  Generate_data_model_2(cc, missing_col1)\n",
        "  cc_data_pat2 =  Generate_data_model_2(cc, missing_col2)\n",
        "  cc_data_pat3 =  Generate_data_model_2(cc, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "  cc_data_loader = DataLoader(cc_data, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader1 = DataLoader(cc_data_pat1, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader2 = DataLoader(cc_data_pat2, batch_size=Batch_size, shuffle=True)\n",
        "  cc_data_loader3 = DataLoader(cc_data_pat3, batch_size=Batch_size, shuffle=True)\n",
        "  \n",
        "  imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)\n",
        "  imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)\n",
        "  imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device)  \n",
        "\n",
        "  critic = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)    \n",
        "  imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "  imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "  critic_optimizer = optim.Adam(critic.parameters(), lr=clr, betas=(.5, .9))\n",
        "  scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "  scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "  mse_loss = torch.nn.MSELoss(reduction='mean')\n",
        "  l1_loss = nn.L1Loss(reduction='mean')\n",
        "\n",
        "  critic_updates = 0\n",
        "  n_critic = 5\n",
        "\n",
        "  for epoch in range(1,1+epochs):\n",
        "    for cc in cc_data_loader:      \n",
        "      cc = cc.to(device).float()                           \n",
        "      batch_size = cc.shape[0]\n",
        "      impu_noise = torch.empty(batch_size, p_f+1, device=device)\n",
        "     \n",
        "      cc_1, mask1 = next(iter(cc_data_loader1))\n",
        "      cc_1 = cc_1[0:batch_size].to(device).float()\n",
        "      mask1 = mask1[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "      cc_hat1 = cc_1.clone()\n",
        "      cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "\n",
        "      cc_2, mask2 = next(iter(cc_data_loader2))\n",
        "      cc_2 = cc_2[0:batch_size].to(device).float()\n",
        "      mask2 = mask2[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "      cc_hat2 = cc_2.clone()\n",
        "      cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2] \n",
        "\n",
        "      cc_3, mask3 = next(iter(cc_data_loader3))\n",
        "      cc_3 = cc_3[0:batch_size].to(device).float()\n",
        "      mask3 = mask3[0:batch_size].to(device).float()\n",
        "      impu_noise.normal_(mean=0,std=1)\n",
        "      imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "      cc_hat3 = cc_3.clone()\n",
        "      cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      \n",
        "      update_critic = CriticUpdater_model_2(critic, critic_optimizer, batch_size, device=device, gp_lambda=gp_lambda)\n",
        "      update_critic(cc, cc_hat1, cc_hat2, cc_hat3)\n",
        "      critic_updates += 1\n",
        "\n",
        "      if critic_updates == n_critic:\n",
        "        critic_updates = 0\n",
        "        reconstruction_loss = 0\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data1 = imputer1(cc_1, mask1, impu_noise)\n",
        "        cc_hat1 = cc_1.clone()\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        reconstruction_loss += l1_loss(imputed_data1, cc_1.float())\n",
        "        \n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data2 = imputer2(cc_2, mask2, impu_noise)\n",
        "        cc_hat2 = cc_2.clone()\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "        reconstruction_loss += l1_loss(imputed_data2, cc_2.float())\n",
        "\n",
        "        impu_noise.normal_(mean=0,std=1)\n",
        "        imputed_data3 = imputer3(cc_3, mask3, impu_noise)\n",
        "        cc_hat3 = cc_3.clone()\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        reconstruction_loss += l1_loss(imputed_data3, cc_3.float())\n",
        "\n",
        "        loss = -critic(cc_hat1).mean()-critic(cc_hat2).mean()-critic(cc_hat3).mean() + weight*reconstruction_loss\n",
        "        \n",
        "        imputer1.zero_grad()\n",
        "        imputer2.zero_grad()\n",
        "        imputer3.zero_grad()\n",
        "        loss.backward()\n",
        "        imputer_optimizer.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imputer1.eval()\n",
        "    imputer2.eval()\n",
        "    imputer3.eval()\n",
        "    inc_data_pat1 = torch.from_numpy(rawdata_inc1).to(device)\n",
        "    inc_data_pat2 = torch.from_numpy(rawdata_inc2).to(device)\n",
        "    inc_data_pat3 = torch.from_numpy(rawdata_inc3).to(device)\n",
        "    mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "    mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "    mask3 = torch.from_numpy(np.logical_and(missing_col1,missing_col2)).to(device)\n",
        "    for i in range(1):\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,np.logical_not(missing_row2))), p_f+1, device=device)\n",
        "      imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "      rawdata[np.logical_and(missing_row1,np.logical_not(missing_row2))] = rawdata_inc1.copy()\n",
        "      \n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row2,np.logical_not(missing_row1))), p_f+1, device=device)\n",
        "      imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "      rawdata[np.logical_and(missing_row2,np.logical_not(missing_row1))] = rawdata_inc2.copy()\n",
        "\n",
        "      impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "      imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "      rawdata_inc3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "      rawdata[np.logical_and(missing_row1, missing_row2)] = rawdata_inc3.copy()\n",
        "  \n",
        "  alist = []\n",
        "  for j in range(2+MI):\n",
        "    rawdata_pat1 = rawdata[np.logical_and(missing_row1,missing_row2==False),:].copy()\n",
        "    rawdata_pat2 = rawdata[np.logical_and(missing_row1==False,missing_row2),:].copy()\n",
        "    rawdata_pat3 = rawdata[np.logical_and(missing_row1,missing_row2),:].copy()\n",
        "    train_pat1 = rawdata[np.logical_or(missing_row1==False,missing_row2),:].copy()\n",
        "    train_pat1 = Generate_data_model_2(train_pat1, missing_col1)\n",
        "    train_pat2 = rawdata[np.logical_or(missing_row1,missing_row2==False),:].copy()\n",
        "    train_pat2 =  Generate_data_model_2(train_pat2, missing_col2)\n",
        "    train_pat3 = rawdata[np.logical_or(missing_row1==False,missing_row2==False),:].copy()\n",
        "    train_pat3 =  Generate_data_model_2(train_pat3, np.logical_or(missing_col1,missing_col2))\n",
        "\n",
        "    train_pat1_loader1 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat1_loader2 = DataLoader(train_pat1, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader1 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat2_loader2 = DataLoader(train_pat2, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat3_loader1 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "    train_pat3_loader2 = DataLoader(train_pat3, batch_size=Batch_size, shuffle=True)\n",
        "\n",
        "    imputer1 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic1 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)  \n",
        "    imputer2 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic2 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    imputer3 = Imputer_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=p_f+1).to(device) \n",
        "    critic3 = Critic_model_2(n1=p_f+1, n2=p_f+1, n3=p_f+1, n4=1).to(device)\n",
        "    \n",
        "    imputer_params = list(imputer1.parameters()) + list(imputer2.parameters()) + list(imputer3.parameters())\n",
        "    critic_params = list(critic1.parameters()) + list(critic2.parameters()) + list(critic3.parameters())\n",
        "    imputer_optimizer = optim.Adam(imputer_params, lr=ilr, betas=(.5, .9))\n",
        "    critic_optimizer = optim.Adam(critic_params, lr=clr, betas=(.5, .9))\n",
        "    scheduler1 = optim.lr_scheduler.StepLR(imputer_optimizer, step_size=10, gamma=gamma)\n",
        "    scheduler2 = optim.lr_scheduler.StepLR(critic_optimizer, step_size=10, gamma=gamma)\n",
        "\n",
        "    critic_updates = 0\n",
        "    for epoch in range(1,1+epochs):\n",
        "      for cc_1, mask1 in train_pat1_loader1:      \n",
        "        cc_1 = cc_1.to(device).float()                           \n",
        "        batch_size1 = cc_1.shape[0]\n",
        "        impu_noise1 = torch.empty(batch_size1, p_f+1, device=device)\n",
        "      \n",
        "        cc_hat1, mask1 = next(iter(train_pat1_loader2))\n",
        "        cc_hat1 = cc_hat1[0:batch_size1].to(device).float()\n",
        "        mask1 = mask1[0:batch_size1].to(device).float()\n",
        "        imputed_data1 = imputer1(cc_hat1, mask1, impu_noise1.normal_(mean=0,std=1))\n",
        "        cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "        \n",
        "        cc_2, mask2 = next(iter(train_pat2_loader1))\n",
        "        batch_size2 = cc_2.shape[0]\n",
        "        impu_noise2 = torch.empty(batch_size2, p_f+1, device=device)\n",
        "        cc_2 = cc_2[0:batch_size2].to(device).float()\n",
        "        cc_hat2, mask2 = next(iter(train_pat2_loader2))\n",
        "        cc_hat2 = cc_hat2[0:batch_size2].to(device).float()\n",
        "        mask2 = mask2[0:batch_size2].to(device).float()\n",
        "        imputed_data2 = imputer2(cc_hat2, mask2, impu_noise2.normal_(mean=0,std=1))\n",
        "        cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "        \n",
        "        cc_3, mask3 = next(iter(train_pat3_loader1))\n",
        "        batch_size3 = cc_3.shape[0]\n",
        "        impu_noise3 = torch.empty(batch_size3, p_f+1, device=device)\n",
        "        cc_3 = cc_3[0:batch_size3].to(device).float()\n",
        "        cc_hat3, mask3 = next(iter(train_pat3_loader2))\n",
        "        cc_hat3 = cc_hat3[0:batch_size3].to(device).float()\n",
        "        mask3 = mask3[0:batch_size3].to(device).float()\n",
        "        imputed_data3 = imputer3(cc_hat3, mask3, impu_noise3.normal_(mean=0,std=1))\n",
        "        cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "\n",
        "        update_critic = CriticUpdater_model_3(critic1, critic2, critic3, critic_optimizer, batch_size1, batch_size2, batch_size3, device=device, gp_lambda=gp_lambda)\n",
        "        update_critic(cc_1, cc_2, cc_3, cc_hat1, cc_hat2, cc_hat3)\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "          critic_updates = 0\n",
        "          reconstruction_loss = 0\n",
        "\n",
        "          imputed_data1 = imputer1(cc_hat1, mask1, impu_noise1.normal_(mean=0,std=1))\n",
        "          cc_hat1[:,missing_col1] = imputed_data1[:,missing_col1]\n",
        "          reconstruction_loss += l1_loss(imputed_data1, cc_1.float())\n",
        "          \n",
        "          imputed_data2 = imputer2(cc_hat2, mask2, impu_noise2.normal_(mean=0,std=1))\n",
        "          cc_hat2[:,missing_col2] = imputed_data2[:,missing_col2]\n",
        "          reconstruction_loss += l1_loss(imputed_data2, cc_2.float())\n",
        "          \n",
        "          imputed_data3 = imputer3(cc_hat3, mask3, impu_noise3.normal_(mean=0,std=1))\n",
        "          cc_hat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data3[:,np.logical_or(missing_col1,missing_col2)]\n",
        "          reconstruction_loss += l1_loss(imputed_data3, cc_3.float())\n",
        "\n",
        "          loss = -critic1(cc_hat1).mean()-critic2(cc_hat2).mean()-critic3(cc_hat3).mean() + weight*reconstruction_loss\n",
        "          imputer_optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          imputer_optimizer.step()\n",
        "      scheduler1.step()\n",
        "      scheduler2.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      imputer1.eval()\n",
        "      imputer2.eval()\n",
        "      imputer3.eval()\n",
        "      inc_data_pat1 = torch.from_numpy(rawdata_pat1).to(device)\n",
        "      inc_data_pat2 = torch.from_numpy(rawdata_pat2).to(device)\n",
        "      inc_data_pat3 = torch.from_numpy(rawdata_pat3).to(device)\n",
        "      mask1 = torch.from_numpy(missing_col1).to(device)\n",
        "      mask2 = torch.from_numpy(missing_col2).to(device)\n",
        "      mask3 = torch.from_numpy(np.logical_or(missing_col1,missing_col2)).to(device)\n",
        "      for i in range(1):\n",
        "        torch.manual_seed(j)\n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2==False)), p_f+1, device=device)\n",
        "        imputed_data = imputer1(inc_data_pat1.float(), mask1.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat1[:,missing_col1] = imputed_data.cpu().numpy()[:,missing_col1]\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2==False)] = rawdata_pat1.copy()\n",
        "        \n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1==False,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer2(inc_data_pat2.float(), mask2.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat2[:,missing_col2] = imputed_data.cpu().numpy()[:,missing_col2]\n",
        "        rawdata[np.logical_and(missing_row1==False,missing_row2)] = rawdata_pat2.copy()\n",
        "        \n",
        "        impu_noise = torch.empty(sum(np.logical_and(missing_row1,missing_row2)), p_f+1, device=device)\n",
        "        imputed_data = imputer3(inc_data_pat3.float(), mask3.float(), impu_noise.normal_(mean=0,std=1)) \n",
        "        rawdata_pat3[:,np.logical_or(missing_col1,missing_col2)] = imputed_data.cpu().numpy()[:,np.logical_or(missing_col1,missing_col2)]\n",
        "        rawdata[np.logical_and(missing_row1,missing_row2)] = rawdata_pat3.copy()\n",
        "\n",
        "    if j > 1:\n",
        "      alist.append(rawdata.copy())\n",
        "  return alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pkeyxyw0UA7"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5oke3JSOYvK"
      },
      "source": [
        "Model_2_SD=[]\n",
        "Model_2_SE_list=[]\n",
        "Model_2_SE=0\n",
        "Model_2_MSE=0\n",
        "Model_2_bias=0\n",
        "Model_2_coverage=0\n",
        "Model_2_time=0\n",
        "\n",
        "a0=1\n",
        "a1=-2\n",
        "a2=3\n",
        "a3=0\n",
        "a4=2\n",
        "a5=-2\n",
        "rho=0.95\n",
        "sigma1=0.1\n",
        "sigma2=0.5\n",
        "Monte_Carlo =100\n",
        "MI=10\n",
        "batch_size=256\n",
        "ilr=1e-3 \n",
        "clr=1e-3 \n",
        "gamma=0.9\n",
        "weight=0.1 \n",
        "gp_lambda=10 \n",
        "epochs=300\n",
        "\n",
        "for iteration in range(1,1+Monte_Carlo):\n",
        "  obs,mis1,mis2,missing_row1,missing_row2,missing_col1,missing_col2,y=data_G_linear(n=n,p=p,true_beta=true_beta,seed=iteration,a0=a0,a1=a1,a2=a2,a3=a3,a4=a4,a5=a5,rho=rho,sigma1=sigma1,sigma2=sigma2)\n",
        "  truth1=mis1[missing_row1==True,:].copy()\n",
        "  truth2=mis2[missing_row2==True,:].copy()\n",
        "\n",
        "  mis1[missing_row1==True,:]=0\n",
        "  mis2[missing_row2==True,:]=0\n",
        "  rawdata = np.concatenate((obs,mis1,mis2,y),axis=1).copy()\n",
        "  \n",
        "  s=time.time()\n",
        "  alist=[]\n",
        "  for i in range(MI):\n",
        "    data = mlp_MIGAN1(rawdata,missing_row1,missing_row2,missing_col1,missing_col2,n=n,p_f=p,Batch_size=batch_size,ilr=ilr,clr=clr,gamma=gamma,weight=weight,gp_lambda=gp_lambda,epochs=epochs,MI_seed=i)[0]\n",
        "    alist.append(data)\n",
        "\n",
        "  # alist = mlp_MIGAN1_iterative2(rawdata,missing_row1,missing_row2,missing_col1,missing_col2,n=n,p_f=p,Batch_size=batch_size,ilr=ilr,clr=clr,gamma=gamma,weight=weight,gp_lambda=gp_lambda,epochs=epochs,MI=MI)\n",
        "\n",
        "  mse,b,c,se=data_to_stats(alist,true_beta,truth1,truth2,missing_row1,missing_row2,missing_col1,missing_col2)\n",
        "  bias,cover=Performance(b,c,true_beta,alpha)\n",
        "  print('Model_2 cost ',time.time()-s,' sec \\n','MSE:', mse,'; Bias: ',bias,'; Coverage: ',cover)\n",
        "  Model_2_SE_list.append(se)\n",
        "  Model_2_SE=(Model_2_SE*(iteration-1)+se)/iteration\n",
        "  Model_2_SD.append(b[0])\n",
        "  Model_2_MSE=(Model_2_MSE*(iteration-1)+mse)/iteration\n",
        "  Model_2_bias=(Model_2_bias*(iteration-1)+bias)/iteration\n",
        "  Model_2_coverage=(Model_2_coverage*(iteration-1)+cover)/iteration\n",
        "  Model_2_time = (Model_2_time*(iteration-1)+time.time()-s)/iteration\n",
        "\n",
        "  if iteration%5==0:\n",
        "    print('iteration: ',iteration, 'Model_2_MSE: ',Model_2_MSE,'Model_2_bias: ',Model_2_bias,'Model_2_coverage: ',Model_2_coverage,'Model_2_SE: ',Model_2_SE)\n",
        "    \n",
        "    Model_2_StD=np.std(Model_2_SD, dtype=np.float64)\n",
        "    print('Model_2_StD: ',Model_2_StD)\n",
        "    Model_2_SE_median=statistics.median(Model_2_SE_list)\n",
        "    print('Model_2 average cost:', Model_2_time/MI)\n",
        "\n",
        "print('n:', n, ' p: ',p, ' Monte Carlo:',Monte_Carlo, 'MI:',MI,'batch_size:',batch_size,'ilr:',ilr,'clr:',clr,'gamma:',gamma,'weight:',weight,'gp_lambda:',gp_lambda,'epochs:',epochs)\n",
        "print('a0=',a0,' a1=', a1,' a2=',a2,' a3=',a3,' a4=',a4,' a5=',a5,' rho=',rho,' sigma1=',sigma1,' sigma2=',sigma2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}